# -*- coding: utf-8 -*-
# Cell 3 — Monthly WF backtest (with ensemble + forward-leak fix)
# Adds: PCA-conditioned covariance, liquidity-aware per-name caps (ADV/AUM),
# one-shot trade caps, PC1 risk nudge, and capacity-aware ERC with vector caps.
# Also: global cap enforcement + breadth relaxation under risk/capacity shortfall.
# NEW: IC-EWMA ensemble weighting, extra-shallow learner, DART learner, alpha-first breadth ranking.

# =============================
# CONFIG
# =============================
CFG3 = {
    "horizons": [1, 2],
    "windows": {"train_months": 144, "valid_months": 24, "calib_months": 24},
    "lgb": {
        "base_params": {
            "learning_rate":    0.03,
            "num_leaves":       63,
            "min_data_in_leaf": 600,
            "feature_fraction": 0.85,
            "bagging_fraction": 0.85,
            "bagging_freq":     1,
            "max_depth":        -1,
            "n_estimators":     1200,
            "subsample":        0.85,
            "n_jobs":           8
        },
        "early_stop_checkpoints": [60,120,150,180,210,240,360,480,600,720,780,840,900,960,1020,1080,1200]
    },
    "ensemble": {
        "enable": True,
        # CHANGED: expanded learner set (added extra_shallow + dart)
        "learners": [
            {"name":"lgb_s0","seed":0,"feature_fraction":0.85,"bagging_fraction":0.85,"num_leaves":63,"max_depth":-1},
            {"name":"lgb_s1","seed":1,"feature_fraction":0.80,"bagging_fraction":0.80,"num_leaves":63,"max_depth":-1},
            {"name":"lgb_s2","seed":2,"feature_fraction":0.90,"bagging_fraction":0.85,"num_leaves":95,"max_depth":-1},
            {"name":"lgb_deep","seed":3,"feature_fraction":0.85,"bagging_fraction":0.85,"num_leaves":127,"max_depth":-1},
            {"name":"lgb_shallow","seed":4,"feature_fraction":0.75,"bagging_fraction":0.75,"num_leaves":31,"max_depth":10},
            # NEW: extra-shallow learner for noisy small-cap alpha
            {"name":"lgb_extra_shallow","seed":5,"feature_fraction":0.65,"bagging_fraction":0.70,"num_leaves":8,"learning_rate":0.10,"max_depth":6},
            # NEW: DART learner for dropout decorrelation
            {"name":"lgb_dart","seed":6,"feature_fraction":0.75,"bagging_fraction":0.80,"num_leaves":63,"max_depth":-1,
             "boosting_type":"dart","drop_rate":0.10,"skip_drop":0.5,"max_drop":50}
        ],
        "weighting": {
            "ic_floor":0.05, "std_eps":1e-6, "softmax_temp":3, "min_weight":0.05,
            # NEW: EWMA half-life (months) for per-learner monthly IC smoothing
            "ewma_halflife_m": 6
        },
        "quantile": {"use_vincentization": True, "vinc_q": 0.50}
    },
    "uncertainty": {"quantiles": [0.10, 0.90], "conformal_alpha": 0.10},
    "confidence": {"conf_scale_base": 1.0},
    "portfolio": {"min_weight_pct":0.1,"max_cash_pct":3.0,"names_min":50,"names_max":80,"names_target":60},
    "universe": {"blacklist": [
        "INE814H01011","INE267A01025","INE364U01010","INE118A01012","INE565A01014","INE415G01027","INE881D01027",
        "INE399L01023","INE619A01035","INE669E01016","INE473A01011","INE188A01015","INE093A01041","INE179A01014",
        "INE691A01018","INE298J01013","INE379A01028","INE457A01014","INE483A01010","INE206F01022","INE672A01018","INE474Q01031"
    ]},
    "allocation": {
        "method": "erc",
        "erc": {"rb_gamma":3,"rb_eps":1e-8,"w_max_pct":4,"max_iter":100000,"tol":5e-4,"step":0.001,"verbose":False},
        "erc_prestress": {"rb_gamma":1.5,"w_max_pct":4},
        "erc_stress": {"rb_gamma":4,"w_max_pct":2},
        # helper knobs
        "breadth_step": 5,                # how many names to add per iteration when relaxing breadth
        "vol_underutil_tol": 0.02         # consider "under vol budget" if below (1 - tol) * target
    },
    "risk": {
        "cov_lookback_m": 24, "cov_shrink_rho": 0.25,
        "vol_target_ann": 0.15, "require_vol_target_ready": True,
        "vol_target_mode": "dynamic",
        "dynamic_vol": {
            "method": "ewma_bench","halflife_m":3,"floor":0.06,"cap":0.28,
            "mult": {"normal":1.15,"pre-stress":0.95,"stress":0.5},
            "smooth_lambda":0.2, "dd_shock":{"lookback_m":6,"threshold":0.10,"shrink":0.50}
        }
    },
    "residuals": {"beta_window_m": 6, "min_obs": 6, "use_resid_sigma_in_scoring": True},
    "scoring": {
        "risk_window_m":6,"min_obs":6,"epsilon":1e-6,"annualize_sigma":True,
        "winsor":{"enable":True,"lower":0.01,"upper":0.99},
        "fallbacks":{"sigma_ann":0.20,"beta":1.0},
        "beta_use_abs": True
    },
    "scoring_map": {"normal":"mu_over_sigma","pre-stress":"ignored_by_blend","stress":"mu_over_sigma"},
    "beta_fill": {
        "normal":{"enable":True,"allow_universe":True,"rel_min":0.01,"abs_min":0.001},
        "pre_stress":{"enable":True,"allow_universe":False,"rel_min":0.05,"abs_min":0.0025}
    },
    "fusion": {"normal":{"alpha":4},"pre_stress":{"alpha":5},"stress":{"alpha":1.1,"all_to_shortest":True}},
    "regimes": {
        "detection":{"mode":"absolute","absolute":{"risk_off_vol_thresh":0.30,"risk_off_drop":-0.025},"logic":"or"},
        "hysteresis":{"enter_need_consecutive":1,"exit_need_consecutive":1,"min_dwell_stress":3,"min_dwell_prestress":1},
        "stress":{"train_weight":2.0,"valid_weight":2.0,"actions":{"vol_target_ann":0.14,"cash_boost_pct":2},
                  "breadth":{"enable":True,"names_min":50,"names_max":90},
                  "beta_backstop":{"enable":False,"min_rel_gap":0.2,"bench_mom_h":3,"max_mix":0.25}},
        "ml":{"enable":True,"tau_in":0.60,"tau_out":0.30,"min_obs":50,"min_pos":12,"min_neg":12,"calib_months":12,
              "require_to_go_live":True,"label_logic_train":"and","vol_thresh_train":0.15,"drop_thresh_train":-0.03,
              "use_scale_pos_weight":True}
    },
    "prestress": {
        "underperf_threshold":0,"consecutive_enter":2,"force_after_stress":False,"force_before_stress":False,
        "blend":{"lam_hi_cooldown":0.60,"lam_lo_deterior":0.30,"lam_step_max":0.10,"lam_bounds":[0.2,0.70],
                 "factor_nudge":0.1,"vol_high_clamp":0.35,"nudge_requires_vol_below_thresh":True},
        "override":{"ic_level_good":0.1,"horizon_corr_good":0.60,"conf_width_max":0.60,"require_at_least":3}
    },
    "costs": {"tc_oneway_bps": 4.0},
    "dates": {"use_date_filter": False, "start_date": None, "end_date": None},
    "gating": {"require_iso_before_live": True},
    "logging": {"debug_first_only": True, "progress_every": 3, "print_each_trade": True}
}

# === New: risk-PCA and liquidity overrides (safe defaults if Cell 2 CFG not present) ===
CFG3.setdefault("risk_pca", {
    "var_explain_target": 0.85,           # τ: explained-variance threshold for PC set size k
    "eig_cap_ratio": 10.0,                # η: cap top eigenvalues relative to mean eigenvalue
    "eig_floor_eps": 1e-8,                # ε: floor for small eigenvalues (× mean eigenvalue)
    "liq_diag": {"gamma": 1.0, "lambda": 1.0},  # liquidity diagonal inflation: scale ~ (medianADV/ADV)^lambda
    "post_nudge": {"pc1_rc_cap": 0.40, "trust_radius_l1": 0.10, "iters": 2}
})
CFG3.setdefault("liquidity", {
    "trade_fraction_one_shot": None,
    "adv_lookback_days": None,
    "adv_floor_rupees": None
})

import os, hashlib, numpy as np, pandas as pd, lightgbm as lgb

from sklearn.isotonic import IsotonicRegression
try:
    from scipy.stats import kendalltau as _kendalltau
except Exception:
    _kendalltau = None

# === Required inputs from Cells 1–2 ===
_need = ["X_all", "targets_h", "ret_m", "mkt_ret_m", "dates_all", "month_constituents"]
for n in _need:
    if n not in globals(): raise RuntimeError(f"Cell 3 needs '{n}' from Cell 2.")
_has_regime_inputs = "idx_prev_ann_vol" in globals()
if not _has_regime_inputs:
    idx_prev_ann_vol = pd.Series(index=ret_m.index, dtype=float)

# === Cell 2 liquidity context (safely read or default) ===
_CFG_cell2 = globals().get("CFG", {}) if isinstance(globals().get("CFG", {}), dict) else {}
_liq_cell2 = _CFG_cell2.get("liquidity", {}) if isinstance(_CFG_cell2.get("liquidity", {}), dict) else {}
_AUM_RS = float(globals().get("AUM_RS", _CFG_cell2.get("aum_rs", 5_000_000_000.0)))
_ADV_FR_pos = float(globals().get("ADV_FR", _liq_cell2.get("adv_fraction_one_shot", 0.20)))
_ADV_LOOK = int(_liq_cell2.get("adv_window_days", 20))
_ADV_FLOOR = float(_liq_cell2.get("adv_floor_rupees", 2_500_000.0))

# === Allow CFG3 overrides for trade caps and ADV params (robust to None) ===
_cfg3_liq = CFG3.get("liquidity", {}) if isinstance(CFG3.get("liquidity", {}), dict) else {}
_trade_FR = _cfg3_liq.get("trade_fraction_one_shot", None)
if _trade_FR is None:
    _trade_FR = _ADV_FR_pos
_adv_lb_override = _cfg3_liq.get("adv_lookback_days", None)
_ADV_LOOK_eff = int(_adv_lb_override) if (_adv_lb_override is not None) else int(_ADV_LOOK)
_adv_floor_override = _cfg3_liq.get("adv_floor_rupees", None)
_ADV_FLOOR_eff = float(_adv_floor_override) if (_adv_floor_override is not None) else float(_ADV_FLOOR)


def C(path, default=None):
    cur = CFG3
    for k in path.split("."):
        if isinstance(cur, dict) and (k in cur): cur = cur[k]
        else: return default
    return cur

_threads = int(os.environ.get("LIGHTGBM_NUM_THREADS",
                os.environ.get("OMP_NUM_THREADS", os.cpu_count() or 8)))
CFG3["lgb"]["base_params"]["n_jobs"] = _threads
CFG3["lgb"]["base_params"]["num_threads"] = _threads

def _p(msg): print(msg, flush=True)

def _bench(dt: pd.Timestamp) -> float:
    b = mkt_ret_m.get(dt, np.nan); return float(b) if np.isfinite(b) else 0.0

# Costs and base gross
tc_oneway = C("costs.tc_oneway_bps") / 1e4
base_gross_min = 1.0 - C("portfolio.max_cash_pct")/100.0
w_floor        = C("portfolio.min_weight_pct")/100.0

# === Math utilities ===

def ann_stats(r):
    if len(r)==0: return np.nan,np.nan,np.nan
    cum = (1.0+r).prod(); yrs = len(r)/12.0
    cagr = cum**(1/yrs)-1.0 if yrs>0 else np.nan
    vol  = r.std(ddof=1)*np.sqrt(12) if len(r)>1 else np.nan
    eq=(1.0+r).cumprod(); peak=eq.cummax(); mdd=(eq/peak-1.0).min()
    return cagr, vol, mdd

def _stable_hash_noise(ids, eps=1e-12):
    out = np.empty(len(ids), dtype=np.float64)
    for i, s in enumerate(ids):
        h = hashlib.blake2b(str(s).encode('utf-8'), digest_size=8).digest()
        u = int.from_bytes(h, 'little')
        out[i] = (u / float(1<<64)) * eps
    return out

def rank_ic_tieaware(x, y, ids=None, eps=1e-12):
    x = np.asarray(x, dtype=np.float64); y = np.asarray(y, dtype=np.float64)
    m = np.isfinite(x) & np.isfinite(y)
    if m.sum() < 6: return np.nan
    x = x[m]; y = y[m]
    ids = (np.arange(len(x)) if ids is None else np.asarray(ids)[m])
    noise = _stable_hash_noise(ids, eps=eps)
    xr = pd.Series(x + noise).rank(method="average").to_numpy(np.float64)
    yr = pd.Series(y + noise).rank(method="average").to_numpy(np.float64)
    sx = xr.std(ddof=1); sy = yr.std(ddof=1)
    if sx > 0 and sy > 0:
        return float(((xr - xr.mean())*(yr - yr.mean())).sum() / ((len(xr)-1)*sx*sy))
    if _kendalltau is not None and len(x) >= 6:
        tau, _ = _kendalltau(x, y, nan_policy='omit'); return float(tau)
    return np.nan

def _h_fwd_return_series(x: pd.Series, h: int) -> pd.Series:
    lr = np.log1p(x.astype(float)); fwd_lr = lr.shift(-1).rolling(h, min_periods=h).sum()
    return np.expm1(fwd_lr)

def _shrink_cov(S: np.ndarray, rho: float) -> np.ndarray:
    S = np.nan_to_num(S, nan=0.0, posinf=0.0, neginf=0.0)
    d = np.diag(np.diag(S)); return (1.0 - rho)*S + rho*d

# === PCA conditioning + liquidity diag inflation ===

def _condition_cov_pca(Sigma: np.ndarray,
                        var_explain_target: float = 0.85,
                        eig_cap_ratio: float = 10.0,
                        eig_floor_eps: float = 1e-8):
    lam, U = np.linalg.eigh(Sigma)
    order = np.argsort(lam)[::-1]
    lam = lam[order].astype(np.float64, copy=False)
    U = U[:, order].astype(np.float64, copy=False)
    total = float(np.sum(lam)) if np.isfinite(np.sum(lam)) else 0.0
    if total <= 0.0:
        return Sigma, U, lam
    csum = np.cumsum(lam)
    k = int(np.searchsorted(csum, float(var_explain_target) * total) + 1)
    lam_bar = float(np.mean(lam)) if np.isfinite(np.mean(lam)) else (total / max(1, len(lam)))
    lam_clip = lam.copy()
    lam_clip[:k] = np.minimum(lam_clip[:k], float(eig_cap_ratio) * lam_bar)
    lam_clip[k:] = np.maximum(lam_clip[k:], float(eig_floor_eps) * lam_bar)
    Sigma_pca = (U * lam_clip) @ U.T
    return Sigma_pca, U, lam_clip

def _adv_asof_wrapper(dt, names, lookback_days: int, adv_floor_rupees: float | None):
    if "_adv_value_asof" in globals():
        try:
            return _adv_value_asof(dt, list(names), int(lookback_days), adv_floor_rupees)
        except Exception:
            pass
    if "adv20_m" in globals():
        try:
            s = adv20_m.loc[dt].reindex(names).astype(float)
            s = s.replace([np.inf,-np.inf], np.nan)
            floor = float(adv_floor_rupees if adv_floor_rupees is not None else _ADV_FLOOR_eff)
            return s.fillna(0.0).clip(lower=floor)
        except Exception:
            pass
    floor = float(adv_floor_rupees if adv_floor_rupees is not None else _ADV_FLOOR_eff)
    return pd.Series(floor, index=pd.Index(names, name="ISIN"), dtype=float)

def _liquidity_diagonal_inflate(Sigma: np.ndarray, adv_series: pd.Series,
                                gamma: float = 1.0, power: float = 1.0,
                                adv_floor: float | None = None) -> np.ndarray:
    if adv_series is None or len(adv_series) == 0:
        return Sigma
    v = adv_series.astype(float).replace([np.inf, -np.inf], np.nan)
    med = float(np.nanmedian(v))
    if not np.isfinite(med) or med <= 0.0:
        med = float(np.nanmean(v)) if np.isfinite(np.nanmean(v)) else 1.0
    base = med / v.clip(lower=float(adv_floor) if adv_floor is not None else 1.0)
    alpha = float(gamma) * np.power(base.to_numpy(np.float64), float(power))
    diag = np.diag(Sigma).astype(np.float64, copy=False)
    Delta = np.diag(alpha * diag)
    return Sigma + Delta

def _pc1_share_and_nudge(w: np.ndarray, Sigma: np.ndarray, U: np.ndarray, lam: np.ndarray,
                         cap: float = 0.40, trust_radius: float = 0.10, iters: int = 2):
    if U is None or lam is None or len(lam) == 0:
        return w, None
    u1 = U[:, 0].astype(np.float64, copy=False)
    lam1 = float(lam[0]) if len(lam) else 0.0

    def _share(wv):
        denom = float(wv @ Sigma @ wv)
        num = lam1 * float((u1 @ wv)**2)
        return (num / denom) if denom > 1e-16 else 0.0

    s1 = _share(w)
    if not np.isfinite(s1) or s1 <= float(cap):
        return w, s1

    w_new = w.copy()
    for _ in range(int(iters)):
        d = -u1 * float(u1 @ w_new)
        d = d - np.mean(d)
        if not np.any(np.isfinite(d)):
            break
        step = float(trust_radius) * (np.linalg.norm(w_new, 1) if np.linalg.norm(w_new, 1) > 0 else 1.0)
        w_new = w_new + step * d / (np.linalg.norm(d, 2) + 1e-12)
        s1 = _share(w_new)
        if s1 <= float(cap):
            break
    return w_new, s1

# === Projection & ERC (vector-cap aware, with scalar cap enforcement) ===

def _project_box_sum(w: np.ndarray, target_sum: float, lower: np.ndarray, upper: np.ndarray,
                     weights_for_redistrib: np.ndarray, max_iter: int = 20) -> np.ndarray:
    w = np.clip(w, lower, upper).astype(np.float64, copy=False)
    b = np.maximum(1e-12, np.nan_to_num(weights_for_redistrib, nan=1.0)).astype(np.float64, copy=False)
    for _ in range(max_iter):
        diff = target_sum - np.nansum(w)
        if abs(diff) < 1e-12:
            break
        free = (w > lower + 1e-12) & (w < upper - 1e-12)
        if not np.any(free):
            break
        alloc = b[free]; alloc = alloc / np.sum(alloc)
        w[free] = np.clip(w[free] + diff * alloc, lower[free], upper[free])
    return np.clip(w, lower, upper)

def erc_from_scores(Sigma: np.ndarray, scores: pd.Series, gross_req: float,
                    w_floor_abs: float, w_max_abs: float, gamma: float, eps: float,
                    max_iter: int, tol: float, step: float, verbose: bool=False,
                    w_max_vec_abs: pd.Series | None = None,
                    target_sum_override: float | None = None) -> pd.Series:
    names = scores.index; n = len(names)
    if n == 0:
        return pd.Series(dtype=float)
    sc = np.asarray(np.maximum(0.0, scores.values) + eps, dtype=np.float64)
    sc = sc if sc.sum() > 0 else np.ones_like(sc)
    b = sc**float(gamma); b /= max(1e-16, b.sum())
    w = b.copy()
    wmin = np.full(n, min(1.0, max(0.0, float(w_floor_abs) / max(1e-12, gross_req))), dtype=np.float64)
    if w_max_vec_abs is not None:
        wmax_vec_frac = np.asarray(
            np.maximum(wmin + 1e-12, w_max_vec_abs.reindex(names).fillna(0.0).to_numpy(np.float64) / max(1e-12, gross_req)),
            dtype=np.float64
        )
        wmax_scalar_frac = float(w_max_abs) / max(1e-12, gross_req)
        wmax = np.minimum(wmax_vec_frac, max(wmin.max() + 1e-12, wmax_scalar_frac))
    else:
        wmax = np.full(n, min(1.0, max(wmin.max() + 1e-12, float(w_max_abs) / max(1e-12, gross_req))), dtype=np.float64)

    sum_min, sum_max = float(np.sum(wmin)), float(np.sum(wmax))
    if sum_min > 1.0 - 1e-12:
        wmin *= (1.0 / sum_min); sum_min = 1.0
    target_sum_frac = float(1.0 if target_sum_override is None else max(0.0, min(1.0, target_sum_override)))
    target_sum_frac = min(target_sum_frac, max(sum_min, min(1.0, sum_max)))

    for it in range(int(max_iter)):
        v = Sigma @ w; sigma2 = float(w @ v)
        RC = np.maximum(w * v, 1e-16); tRC = b * sigma2
        ratio = np.clip(tRC / RC, 1e-6, 1e6)
        w = w * np.power(ratio, float(step))
        w = _project_box_sum(w, target_sum_frac, wmin, wmax, b, max_iter=30)
        v = Sigma @ w; sigma2 = float(w @ v); RC = w * v
        err = np.max(np.abs(RC/np.maximum(1e-16, sigma2) - b))
        if err < float(tol):
            break
    return pd.Series(w * float(gross_req), index=names, dtype=float)

# === Existing scaffolding (unchanged parts) ===
dates_master = dates_all.sort_values()
if C("dates.use_date_filter"):
    def _to_month_end(ts_like):
        if ts_like is None or (isinstance(ts_like, str) and ts_like.strip()==""): return None
        s = str(ts_like).strip()
        if len(s)==4 and s.isdigit(): t = pd.Timestamp(f"{s}-01-01")
        elif len(s)==7:              t = pd.Timestamp(f"{s}-01")
        else:                        t = pd.Timestamp(s)
        return t.to_period('M').to_timestamp(how='end')
    start_ts = _to_month_end(C("dates.start_date")); end_ts = _to_month_end(C("dates.end_date"))
    pre_hist = C("windows.train_months") + C("windows.valid_months") + C("windows.calib_months") + C("risk.cov_lookback_m")
    if start_ts is not None: dates_master = dates_master[dates_master >= (start_ts - pd.DateOffset(months=pre_hist))]
    if end_ts   is not None: dates_master = dates_master[dates_master <= end_ts]

if not isinstance(X_all.index, pd.MultiIndex) or X_all.index.names[:2] != ["Date","ISIN"]:
    raise RuntimeError("X_all must have a MultiIndex index with names ['Date','ISIN'].")

def panel_slice(df_mi, start_dt, end_dt):
    idx = df_mi.index
    m = (idx.get_level_values(0) >= start_dt) & (idx.get_level_values(0) <= end_dt)
    return df_mi.loc[m]

# Safe ladders / beta sleeves / misc helpers (mostly unchanged)

def _build_safe_stress_weights(dt, names_idx, gross_req, w_pre_for_carry):
    try: i_dt_cov = ret_m.index.get_loc(dt)
    except Exception: i_dt_cov = -1
    look = int(C("risk.cov_lookback_m"))
    try:
        if i_dt_cov >= look and len(names_idx) >= 2:
            hist = ret_m.iloc[i_dt_cov-look:i_dt_cov].reindex(columns=names_idx).astype(float)
            sig = hist.std(ddof=1) * np.sqrt(12.0)
            sig = sig.replace([0.0, np.inf, -np.inf], np.nan).dropna()
            if len(sig) >= 2:
                nmin = int(C("regimes.stress.breadth.names_min"))
                nmax = int(C("regimes.stress.breadth.names_max"))
                k = min(max(nmin,  min(nmax, len(sig))), len(sig))
                lowvol = sig.sort_values().head(k).index
                S = np.cov(hist[lowvol].values, rowvar=False)
                Sigma = _shrink_cov(S, float(C("risk.cov_shrink_rho")))
                scores = (1.0 / sig.reindex(lowvol)).fillna(0.0)
                erc_base = C("allocation.erc_stress").copy()
                w_erc = erc_from_scores(Sigma=Sigma, scores=scores, gross_req=float(gross_req),
                                        w_floor_abs=float(w_floor), w_max_abs=float(erc_base["w_max_pct"]/100.0),
                                        gamma=float(erc_base["rb_gamma"]), eps=float(C("allocation.erc.rb_eps")),
                                        max_iter=int(C("allocation.erc.max_iter")), tol=float(C("allocation.erc.tol")),
                                        step=float(C("allocation.erc.step")), verbose=bool(C("allocation.erc.verbose")))
                w_full = pd.Series(0.0, index=ret_m.columns); w_full.loc[w_erc.index]=w_erc.values
                wv = w_full.loc[w_erc.index].values
                vol_m = float(np.sqrt(max(1e-16, wv @ Sigma @ wv))); vol_ann_est = vol_m * np.sqrt(12.0)
                return w_full, vol_ann_est, "safe_lowvol_erc"
    except Exception: pass
    try:
        if len(names_idx) >= 2:
            betas_t = beta_ts.loc[dt] if dt in beta_ts.index else pd.Series(C("scoring.fallbacks.beta"), index=ret_m.columns)
            w_beta = betas_t.reindex(names_idx).clip(lower=0.0).replace([np.inf,-np.inf,np.nan], 0.0)
            s = float(w_beta.sum()); w_beta = w_beta / (s if s>0 else 1.0)
            w_beta = w_beta * float(gross_req)
            w_full = pd.Series(0.0, index=ret_m.columns); w_full.loc[w_beta.index] = w_beta.values
            vol_ann_est = np.nan
            try:
                i_dt_cov = ret_m.index.get_loc(dt); look = int(C("risk.cov_lookback_m"))
                hist = ret_m.iloc[i_dt_cov-look:i_dt_cov].reindex(columns=names_idx).astype(float)
                if hist.shape[1] >= 2 and hist.shape[0] >= max(24, look):
                    S = np.cov(hist.values, rowvar=False); Sigma = _shrink_cov(S, float(C("risk.cov_shrink_rho")))
                    wv = w_beta.values; vol_m = float(np.sqrt(max(1e-16, wv @ Sigma @ wv))); vol_ann_est = vol_m * np.sqrt(12.0)
            except Exception: pass
            return w_full, vol_ann_est, "safe_beta_sleeve"
    except Exception: pass
    try:
        w_full = pd.Series(0.0, index=ret_m.columns)
        carry = w_pre_for_carry.reindex(ret_m.columns).fillna(0.0).clip(lower=0.0)
        s = float(carry.sum()); carry = carry / (s if s>0 else 1.0)
        w_full = carry
        return w_full, np.nan, "safe_carry"
    except Exception: pass
    return None, np.nan, "safe_failed"


def _safe_beta_equal_weight(dt, names_idx, from_regime: str):
    gross_req = 1.0 - min(100.0, C("portfolio.max_cash_pct"))/100.0
    betas_t = beta_ts.loc[dt] if dt in beta_ts.index else pd.Series(C("scoring.fallbacks.beta"), index=ret_m.columns)
    b = betas_t.reindex(names_idx).replace([np.inf,-np.inf,np.nan], 0.0)
    pos = b[b > 0].index.tolist()
    tgt = int(C("portfolio.names_target")); nmin = int(C("portfolio.names_min")); nmax = int(C("portfolio.names_max"))
    if len(pos) >= max(2, nmin): use = pos[:min(len(pos), max(nmin, min(tgt, nmax)))]
    else: use = list(names_idx[:max(2, min(tgt, len(names_idx)))])
    if len(use) < 2: return {"status":"fail","fail_reason":"safe_eq_too_narrow"}
    w_safe = pd.Series(0.0, index=ret_m.columns, dtype=float); w_safe.loc[use] = float(gross_req) / float(len(use))
    vol_ann_est = np.nan
    try:
        i_dt_cov = ret_m.index.get_loc(dt); look = int(C("risk.cov_lookback_m"))
        hist = ret_m.iloc[i_dt_cov-look:i_dt_cov].reindex(columns=use).fillna(0.0)
        if hist.shape[0] >= max(24, look) and hist.shape[1] >= 2:
            S = np.cov(hist.values, rowvar=False); Sigma = _shrink_cov(S, float(C("risk.cov_shrink_rho")))
            vol_ann_est = float(np.sqrt(max(1e-16, (w_safe.loc[use].values @ Sigma @ w_safe.loc[use].values)))) * np.sqrt(12.0)
    except Exception: pass
    return {"status":"ok","w": w_safe,"vol_ann": vol_ann_est,"scores_method":"safe_beta_eq",
            "iso_flag":"n/a","note_extra":f"fallback=safe_beta_eq_from_{from_regime}","top_idx": pd.Index(use, dtype=object)}


def _vol_ann_of(w_vec: np.ndarray, Sigma: np.ndarray) -> float:
    v_m = float(np.sqrt(max(1e-16, w_vec @ Sigma @ w_vec))); return v_m * np.sqrt(12.0)


def _mix_to_target_vol(wa: np.ndarray, wb: np.ndarray, Sigma: np.ndarray, vtarget_ann: float):
    def _vol(w): return _vol_ann_of(w, Sigma)
    vol_a = _vol(wa); vol_b = _vol(wb)
    if (not np.isfinite(vtarget_ann)) or vtarget_ann <= 0.0 or vol_a >= vtarget_ann - 1e-6 or vol_b <= vol_a + 1e-9:
        return wa, vol_a
    lo, hi = 0.0, 1.0
    for _ in range(25):
        mid = 0.5*(lo+hi)
        v_mid = _vol((1.0 - mid)*wa + mid*wb)
        if v_mid < vtarget_ann: lo = mid
        else: hi = mid
    w_mix = (1.0 - hi)*wa + hi*wb
    return w_mix, _vol(w_mix)


def _maybe_beta_fill(regime_kind, dt, names_idx, top_idx, Sigma, X_now_df, w_new_full, vtarget_now, gross_req, vol_ann_est):
    if regime_kind not in ("normal", "pre-stress"): return w_new_full, vol_ann_est, ""
    key = "normal" if regime_kind == "normal" else "pre_stress"
    cfg = C(f"beta_fill.{key}")
    if not cfg or not bool(cfg.get("enable", False)): return w_new_full, vol_ann_est, ""
    rel_min = float(cfg.get("rel_min", 0.05)); abs_min = float(cfg.get("abs_min", 0.0025))
    betas_t = beta_ts.loc[dt] if dt in beta_ts.index else pd.Series(C("scoring.fallbacks.beta"), index=ret_m.columns)
    w_beta_act = betas_t.reindex(top_idx).clip(lower=0.0).replace([np.inf,-np.inf,np.nan], 0.0)
    s = float(w_beta_act.sum()); w_beta_act = (w_beta_act / (s if s>0 else 1.0)) * float(gross_req)
    vol_beta_act = _vol_ann_of(w_beta_act.values, Sigma)
    vol_beta_uni = np.nan; w_beta_uni_act = None
    if bool(cfg.get("allow_universe", True)):
        try:
            uni_idx = X_now_df.index.get_level_values(1).unique()
            w_beta_uni = betas_t.reindex(uni_idx).clip(lower=0.0).replace([np.inf,-np.inf,np.nan], 0.0)
            su = float(w_beta_uni.sum()); w_beta_uni = (w_beta_uni / (su if su>0 else 1.0)) * float(gross_req)
            w_beta_uni_act = w_beta_uni.reindex(top_idx).fillna(0.0).values
            vol_beta_uni = _vol_ann_of(w_beta_uni_act, Sigma)
        except Exception: pass
    v_feasible = max([vol_ann_est] + [v for v in [vol_beta_act, vol_beta_uni] if np.isfinite(v)] or [vol_ann_est])
    vtarget_eff = min(float(vtarget_now), v_feasible)
    gap = vtarget_eff - vol_ann_est
    if not (gap > max(abs_min, rel_min * max(vol_ann_est, 1e-12))): return w_new_full, vol_ann_est, ""
    beta_note = ""
    w_alpha = w_new_full.loc[top_idx].values
    w_mix, v_mix = _mix_to_target_vol(w_alpha, w_beta_act.values, Sigma, vtarget_eff)
    if v_mix > vol_ann_est + 1e-9:
        w_new_full.loc[top_idx] = w_mix; vol_ann_est = v_mix; beta_note = "beta=smart"
    if np.isfinite(vol_beta_uni) and (vol_ann_est < vtarget_eff - 1e-6) and (vol_beta_uni > vol_beta_act + 1e-9):
        w_mix2, v_mix2 = _mix_to_target_vol(w_new_full.loc[top_idx].values, w_beta_uni_act, Sigma, vtarget_eff)
        if v_mix2 > vol_ann_est + 1e-9:
            w_new_full.loc[top_idx] = w_mix2; vol_ann_est = v_mix2; beta_note = "beta=universe" if beta_note=="" else beta_note+"+univ"
    return w_new_full, vol_ann_est, beta_note


_dyn_base_vt_prev = None

def _dyn_vol_target(dt: pd.Timestamp, regime_kind: str) -> float:
    mode = str(C("risk.vol_target_mode","fixed")).lower()
    if mode != "dynamic":
        bench_vol_last = float(idx_prev_ann_vol.get(dt, np.nan)) if _has_regime_inputs else np.nan
        base_vt = float(C("risk.vol_target_ann"))
        if regime_kind == "stress": return float(C("regimes.stress.actions.vol_target_ann"))
        elif regime_kind == "pre-stress":
            pre_vt = max(0.12, bench_vol_last if np.isfinite(bench_vol_last) else base_vt); return float(min(base_vt, pre_vt))
        else: return float(max(base_vt, bench_vol_last if np.isfinite(bench_vol_last) else 0.0))
    cfg = C("risk.dynamic_vol", {}); hl = float(cfg.get("halflife_m",6)); floor_v=float(cfg.get("floor",0.08))
    cap_v=float(cfg.get("cap",0.28)); lam=float(cfg.get("smooth_lambda",0.7))
    mult_map = cfg.get("mult", {"normal":1.0,"pre-stress":0.9,"stress":0.75}); mult=float(mult_map.get(regime_kind,1.0))
    try:
        vol_hist = idx_prev_ann_vol.loc[idx_prev_ann_vol.index <= dt].dropna()
        base_raw = float(vol_hist.ewm(halflife=max(0.5,hl), adjust=False, min_periods=1).mean().iloc[-1]) if len(vol_hist) else float(C("risk.vol_target_ann"))
    except Exception:
        base_raw = float(C("risk.vol_target_ann"))
    global _dyn_base_vt_prev
    base_smooth = float(base_raw) if (_dyn_base_vt_prev is None or not np.isfinite(_dyn_base_vt_prev)) else float(lam*_dyn_base_vt_prev+(1-lam)*base_raw)
    _dyn_base_vt_prev = base_smooth
    v_t = base_smooth * mult
    try:
        dd_cfg = cfg.get("dd_shock", {}); L=int(dd_cfg.get("lookback_m",6)); th=float(dd_cfg.get("threshold",0.10)); shrink=float(dd_cfg.get("shrink",0.85))
        hist = mkt_ret_m.loc[mkt_ret_m.index < dt].tail(L).astype(float)
        if len(hist):
            eq = (1.0 + hist.fillna(0.0)).cumprod(); peak = eq.cummax()
            dd = float((eq/peak - 1.0).min())
            if np.isfinite(dd) and (-dd) >= th: v_t *= shrink
    except Exception: pass
    v_t = float(np.clip(v_t, floor_v, cap_v)); return v_t


def _factor_override_strength(mu_plain: dict, qlo_plain: dict, qhi_plain: dict, ic_mean_valid: float):
    cfg = C("prestress.override", {}) 
    if not mu_plain: return False, {"reason":"no_mu"}
    hmin = min(mu_plain.keys()); hmax = max(mu_plain.keys())
    s1 = mu_plain[hmin].astype(float); s2 = mu_plain[hmax].reindex(s1.index).astype(float) if (hmax in mu_plain) else s1.copy()
    try: corr = float(s1.corr(s2, method="spearman"))
    except Exception: corr = float(s1.corr(s2))
    qlo = qlo_plain.get(hmin, pd.Series(index=s1.index, dtype=float)).reindex(s1.index)
    qhi = qhi_plain.get(hmin, pd.Series(index=s1.index, dtype=float)).reindex(s1.index)
    width = (qhi - qlo).abs(); mu_abs = s1.abs().replace(0.0, np.nan)
    width_norm_med = float((width / (mu_abs + 1e-8)).replace([np.inf,-np.inf], np.nan).median())
    ic_good = (np.isfinite(ic_mean_valid) and ic_mean_valid >= float(cfg.get("ic_level_good",0.04)))
    hcorr_ok = (np.isfinite(corr) and corr >= float(cfg.get("horizon_corr_good",0.6)))
    conf_ok = (np.isfinite(width_norm_med) and width_norm_med <= float(cfg.get("conf_width_max",0.6)))
    ok_count = int(ic_good) + int(hcorr_ok) + int(conf_ok); need_at_least=int(cfg.get("require_at_least",1))
    strong = ok_count >= need_at_least
    diag = {"ic_mean":(None if not np.isfinite(ic_mean_valid) else float(ic_mean_valid)),
            "horizon_corr":(None if not np.isfinite(corr) else float(corr)),
            "conf_width_norm_med":(None if not np.isfinite(width_norm_med) else float(width_norm_med)),
            "ok_count":ok_count,"need_at_least":need_at_least,
            "conds":{"ic_good":ic_good,"h_corr_good":hcorr_ok,"conf_narrow":conf_ok}}
    return strong, diag


def _fuse_horizons(series_by_h: dict, regime_family: str) -> pd.Series:
    if not series_by_h: return pd.Series(dtype=float)
    hs = sorted(series_by_h.keys()); hmin = hs[0]
    fcfg = C(f"fusion.{ 'pre_stress' if regime_family=='pre-stress' else regime_family.replace('-','_') }", {})
    if fcfg.get("all_to_shortest", False): return series_by_h[hmin].copy()
    alpha = float(fcfg.get("alpha", 4.0)); weights = {h: alpha ** (-(h - hmin)) for h in hs}; Z = float(sum(weights.values()))
    out = None
    for h in hs:
        s = series_by_h[h]
        out = (weights[h]/Z) * s if out is None else out.add((weights[h]/Z) * s, fill_value=0.0)
    return out if out is not None else pd.Series(dtype=float)


def _rolling_sigma(dt: pd.Timestamp, names_idx, window_m: int, annualize: bool, use_resid_sigma: bool) -> pd.Series:
    try: i = ret_m.index.get_loc(dt)
    except Exception: return pd.Series(float(C("scoring.fallbacks.sigma_ann")), index=pd.Index(names_idx, dtype=object))
    i0 = max(0, i - window_m); hist = ret_m.iloc[i0:i].reindex(columns=names_idx).astype(float)
    if hist.shape[0] < int(C("scoring.min_obs", 3)): return pd.Series(float(C("scoring.fallbacks.sigma_ann")), index=pd.Index(names_idx, dtype=object))
    if use_resid_sigma:
        mkt = mkt_ret_m.iloc[i0:i].astype(float); bet = beta_ts.iloc[i0:i].reindex(columns=names_idx).astype(float)
        resid = hist.sub(bet.mul(mkt, axis=0), axis=0); sig = resid.std(ddof=1)
    else: sig = hist.std(ddof=1)
    if bool(C("scoring.annualize_sigma", True)): sig = sig * np.sqrt(12.0)
    sig = sig.replace([np.inf,-np.inf], np.nan).fillna(float(C("scoring.fallbacks.sigma_ann")))
    return sig.astype(float)


def _winsorize_series(s: pd.Series, lower_q: float, upper_q: float) -> pd.Series:
    if s.empty: return s
    lo = float(np.nanquantile(s.values, lower_q)) if s.notna().sum() >= 10 else s.min()
    hi = float(np.nanquantile(s.values, upper_q)) if s.notna().sum() >= 10 else s.max()
    return s.clip(lower=lo, upper=hi)


def _build_family_scores(dt: pd.Timestamp, names_idx: pd.Index,
                         mu_plain: dict, qlo_plain: dict, qhi_plain: dict,
                         mu_resid: dict, qlo_resid: dict, qhi_resid: dict,
                         family: str):
    fam = "pre_stress" if family=="pre-stress" else family
    choice = str(C("scoring_map." + family)).strip().lower()
    mu_stack = mu_resid if choice in ("mu_over_sigma","mu_over_beta","mu") else mu_resid
    mu_fused = _fuse_horizons({h: mu_stack[h].reindex(names_idx) for h in mu_stack}, fam)
    if C("scoring.winsor.enable", True):
        mu_fused = _winsorize_series(mu_fused, float(C("scoring.winsor.lower", 0.01)), float(C("scoring.winsor.upper", 0.99)))
    eps = float(C("scoring.epsilon", 1e-6))
    if choice == "mu_over_sigma":
        sig = _rolling_sigma(dt, names_idx, int(C("scoring.risk_window_m", 3)),
                             bool(C("scoring.annualize_sigma", True)),
                             bool(C("residuals.use_resid_sigma_in_scoring", True)))
        sc = (mu_fused / (sig.abs() + eps)).replace([np.inf,-np.inf], 0.0).fillna(0.0); return sc.astype(float), "mu_over_sigma"
    elif choice == "mu_over_beta":
        bet = beta_ts.loc[dt].reindex(names_idx); 
        if bool(C("scoring.beta_use_abs", True)): bet = bet.abs()
        bet = bet.replace(0.0, np.nan).fillna(float(C("scoring.fallbacks.beta")))
        sc = (mu_fused / (bet + eps)).replace([np.inf,-np.inf], 0.0).fillna(0.0); return sc.astype(float), "mu_over_beta"
    else:
        return mu_fused.fillna(0.0).astype(float), "mu"


def _vol_ready_for(dt: pd.Timestamp, w_full: pd.Series):
    nz = w_full[w_full > 0].index.tolist()
    if len(nz) < 2: return False, "too_few_names"
    try: i = ret_m.index.get_loc(dt)
    except Exception: return False, "dt_not_in_returns"
    look = int(C("risk.cov_lookback_m", 24)); hist = ret_m.iloc[i-look:i].reindex(columns=nz).astype(float)
    if hist.shape[0] < max(24, look) or hist.shape[1] < 2: return False, "hist_shape"
    if not np.isfinite(hist.values).any(): return False, "non_finite_hist"
    return True, "ok"


def _riskoff_label_and_feats_for(dt: pd.Timestamp):
    vol_prev = float(idx_prev_ann_vol.get(dt, np.nan)) if _has_regime_inputs else np.nan
    r_prev   = float(mkt_ret_m.get(dt, np.nan))
    vol_th_eff  = float(C("regimes.detection.absolute.risk_off_vol_thresh"))
    drop_th_eff = float(C("regimes.detection.absolute.risk_off_drop"))
    if not (np.isfinite(vol_prev) and np.isfinite(r_prev)): return None, None, False
    logic = str(C("regimes.detection.logic","and")).lower()
    cond_vol  = (vol_prev >= vol_th_eff); cond_drop = (r_prev <= drop_th_eff)
    y = 1 if ((cond_vol and cond_drop) if logic=="and" else (cond_vol or cond_drop)) else 0
    x = np.array([vol_prev, r_prev, vol_th_eff, drop_th_eff], dtype=np.float32)
    return int(y), x, True


def _riskoff_label_and_feats_for_train(dt: pd.Timestamp):
    return _riskoff_label_and_feats_for(dt)


def is_risk_off_month(dt: pd.Timestamp):
    vol_prev = float(idx_prev_ann_vol.get(dt, np.nan)) if _has_regime_inputs else np.nan
    r_prev   = float(mkt_ret_m.get(dt, np.nan))
    vol_th_eff  = float(C("regimes.detection.absolute.risk_off_vol_thresh"))
    drop_th_eff = float(C("regimes.detection.absolute.risk_off_drop"))
    logic = str(C("regimes.detection.logic","and")).lower()
    cond_vol  = np.isfinite(vol_prev) and (vol_prev >= vol_th_eff)
    cond_drop = np.isfinite(r_prev)   and (r_prev   <= drop_th_eff)
    risk_off_abs = (cond_vol and cond_drop) if logic=="and" else (cond_vol or cond_drop)
    return bool(risk_off_abs), vol_prev, r_prev, vol_th_eff, drop_th_eff, "absolute"

beta_win = int(C("residuals.beta_window_m")); beta_min = int(C("residuals.min_obs"))
var_mkt_roll = mkt_ret_m.rolling(beta_win, min_periods=beta_min).var(ddof=1)
beta_ts = pd.DataFrame(index=ret_m.index, columns=ret_m.columns, dtype=float)
for col in ret_m.columns:
    cov_im = ret_m[col].rolling(beta_win, min_periods=beta_min).cov(mkt_ret_m)
    beta_ts[col] = cov_im / var_mkt_roll
beta_ts = beta_ts.fillna(C("scoring.fallbacks.beta"))

mkt_h_ret = {h: _h_fwd_return_series(mkt_ret_m, h) for h in C("horizons")}

w_prev_state_book = pd.Series(0.0, index=ret_m.columns)
live_started = False
last_live_regime = None
w_prev_state_dec = pd.Series(0.0, index=ret_m.columns)
dec_last_regime = "normal"
dec_prestress_lambda_prev = 0.50
risk_off_streak = 0
risk_on_streak  = 0
stress_dwell_left = 0
prestress_dwell_left = 0


def _update_dwell_counters(prev_regime: str, new_regime: str):
    global stress_dwell_left, prestress_dwell_left
    if new_regime == "stress":
        if prev_regime != "stress": stress_dwell_left = max(0, int(C("regimes.hysteresis.min_dwell_stress", 2)) - 1)
        else: stress_dwell_left = max(0, stress_dwell_left - 1)
        if prev_regime == "pre-stress": prestress_dwell_left = 0
    elif new_regime == "pre-stress":
        if prev_regime != "pre-stress": prestress_dwell_left = max(0, int(C("regimes.hysteresis.min_dwell_prestress", 1)) - 1)
        else: prestress_dwell_left = max(0, prestress_dwell_left - 1)
        if prev_regime == "stress": stress_dwell_left = 0
    else:
        stress_dwell_left = 0; prestress_dwell_left = 0

weights_by_date = {}; weights_pre_by_date = {}; weights_post_by_date = {}; orders_by_date = {}
dates_traded, ret_gross_m, ret_net_m, turnover_money_m, cost_m = [], [], [], [], []
regime_at_trade_date = {}
pred_log = {}
shadow_dates_traded, shadow_ret_gross_m = [], []
shadow_regime_at_trade_date = {}


def _book_next_month(dt_decision, w_pre, w_post, regime_kind, vol_ann_est_final=np.nan, note=None):
    i_next = list(dates_master).index(dt_decision) + 1
    if i_next >= len(dates_master): return None
    dt_next = dates_master[i_next]
    if dt_next not in ret_m.index: return None

    r_next = ret_m.loc[dt_next].reindex(w_post.index).fillna(0.0)
    rg = float((w_post * r_next).sum())

    money_turn = 0.5 * np.abs(w_post - w_pre).sum()
    cost_real = tc_oneway * money_turn

    ret_gross_m.append(rg)
    ret_net_m.append(rg - cost_real)
    turnover_money_m.append(money_turn)
    cost_m.append(cost_real)
    dates_traded.append(dt_next)

    regime_at_trade_date[dt_next] = regime_kind
    weights_pre_by_date[dt_decision]  = w_pre[w_pre>0].copy()
    weights_post_by_date[dt_decision] = w_post[w_post>0].copy()
    orders_by_date[dt_decision]       = (w_post - w_pre)

    note_str = f" | note={note}" if note else ""
    vol_str = "nan%" if not np.isfinite(vol_ann_est_final) else f"{vol_ann_est_final:.2%}"
    _p(f"{dt_next:%Y-%m}: regime={regime_kind} | excess={rg - _bench(dt_next):+.2%} | vol_ann={vol_str}{note_str}")
    return dt_next, rg


def _carry_no_trade(dt_decision, w_pre, regime_kind, note):
    global live_started
    if not live_started: return None
    return _book_next_month(dt_decision, w_pre, w_pre, regime_kind, np.nan, note)


def _sim_next_month(dt_decision, w_pre, w_post, regime_kind, note=None):
    global w_prev_state_dec
    i_next = list(dates_master).index(dt_decision) + 1
    if i_next >= len(dates_master): return None
    dt_next = dates_master[i_next]
    if dt_next not in ret_m.index: return None
    r_next = ret_m.loc[dt_next].reindex(w_post.index).fillna(0.0)
    rg = float((w_post * r_next).sum())
    shadow_ret_gross_m.append(rg); shadow_dates_traded.append(dt_next)
    shadow_regime_at_trade_date[dt_next] = regime_kind
    w_prev_state_dec = w_post.copy()
    return dt_next, rg


def _decision_carry(dt_decision, w_pre, regime_kind, note):
    return _sim_next_month(dt_decision, w_pre, w_pre, regime_kind, note)

# === LightGBM wrappers ===

def fit_lgb_reg(X, y, params, num_rounds, sample_weight=None):
    dtrain = lgb.Dataset(X, label=y, weight=sample_weight, free_raw_data=False)
    p = params.copy(); p.update(dict(objective="regression", metric="l2", verbose=-1))
    p["num_threads"] = p.get("num_threads", _threads); p["n_jobs"] = p.get("n_jobs", _threads)
    try: return lgb.train(p, dtrain, num_boost_round=num_rounds, callbacks=[lgb.log_evaluation(period=0)])
    except TypeError: return lgb.train(p, dtrain, num_boost_round=num_rounds)

def fit_lgb_quantile(X, y, params, alpha, num_rounds, sample_weight=None):
    dtrain = lgb.Dataset(X, label=y, weight=sample_weight, free_raw_data=False)
    p = params.copy(); p.update(dict(objective="quantile", alpha=float(alpha), metric="quantile", verbose=-1))
    p["num_threads"] = p.get("num_threads", _threads); p["n_jobs"] = p.get("n_jobs", _threads)
    try: return lgb.train(p, dtrain, num_boost_round=num_rounds, callbacks=[lgb.log_evaluation(period=0)])
    except TypeError: return lgb.train(p, dtrain, num_boost_round=num_rounds)

def fit_lgb_bin(X, y, params, num_rounds, sample_weight=None):
    dtrain = lgb.Dataset(X, label=y, weight=sample_weight, free_raw_data=False)
    p = params.copy(); p.update(dict(objective="binary", metric="binary_logloss", verbose=-1))
    if "scale_pos_weight" not in p: p["is_unbalance"] = True
    p["num_threads"] = p.get("num_threads", _threads); p["n_jobs"] = p.get("n_jobs", _threads)
    try: return lgb.train(p, dtrain, num_boost_round=num_rounds, callbacks=[lgb.log_evaluation(period=0)])
    except TypeError: return lgb.train(p, dtrain, num_boost_round=num_rounds)


def _riskoff_train_example_ok(dt: pd.Timestamp):
    vol_prev = float(idx_prev_ann_vol.get(dt, np.nan)) if _has_regime_inputs else np.nan
    r_prev   = float(mkt_ret_m.get(dt, np.nan))
    return np.isfinite(vol_prev) and np.isfinite(r_prev)


def _train_and_score_stress_clf(end_train_dt: pd.Timestamp, score_dt: pd.Timestamp):
    months_hist = [d for d in ret_m.index if d < end_train_dt]
    ys, Xs = [], []
    for m in months_hist:
        y_tr, x, ok = _riskoff_label_and_feats_for_train(m)
        if ok: ys.append(y_tr); Xs.append(x)
    if len(ys) == 0: return np.nan, "off", False, {"pos":0,"neg":0,"obs":0}
    ys = np.asarray(ys, dtype=np.float32); Xs = np.asarray(Xs, dtype=np.float32)
    # CHANGED: fix unpack bug and compute pos/neg/obs cleanly
    pos = int((ys==1).sum()); neg = int((ys==0).sum()); obs = int(len(ys))
    cfg = C("regimes.ml", {})
    if (obs < int(cfg.get("min_obs", 36))) or (pos < int(cfg.get("min_pos", 6))) or (neg < int(cfg.get("min_neg", 6))):
        return np.nan, "off", False, {"pos":pos,"neg":neg,"obs":obs}
    cp = max(C("lgb.early_stop_checkpoints"))
    try:
        ages = np.arange(len(ys), dtype=np.float32)[::-1]
        w_rec = np.power(0.5, ages / 6.0).astype(np.float32)
    except Exception:
        w_rec = None
    params_bin = C("lgb.base_params").copy()
    if bool(cfg.get("use_scale_pos_weight", True)) and pos > 0:
        params_bin["scale_pos_weight"] = max(1.0, float(neg) / float(pos))
    model = fit_lgb_bin(Xs, ys, params_bin, num_rounds=cp, sample_weight=w_rec)
    calib_m = int(cfg.get("calib_months", 12)); iso = None; det_iso_flag = "off"
    if obs >= max(calib_m+4, 12):
        try:
            X_cal = Xs[-calib_m:] if calib_m>0 else Xs; y_cal = ys[-calib_m:] if calib_m>0 else ys
            p_cal = model.predict(X_cal)
            if (y_cal.min() != y_cal.max()):
                iso = IsotonicRegression(out_of_bounds="clip").fit(p_cal, y_cal); det_iso_flag = "on"
        except Exception:
            iso = None; det_iso_flag = "off"
    y_now, x_now, ok_now = _riskoff_label_and_feats_for(score_dt)
    if not ok_now: return np.nan, "off", False, {"pos":pos,"neg":neg,"obs":obs}
    p_raw = float(model.predict(x_now.reshape(1,-1))[0]); p_hat = float(iso.predict([p_raw])[0]) if iso is not None else p_raw
    return p_hat, det_iso_flag, True, {"pos":pos,"neg":neg,"obs":obs}

# ===== Ensemble helpers =====

def _softmax_vec(x, temp=1.0):
    x = np.asarray(x, dtype=np.float64); x = x / max(temp, 1e-8)
    x = x - np.nanmax(x); ex = np.exp(x); s = ex.sum()
    return ex / max(s, 1e-12)


def _ic_weights(ic_vec, cfg):
    ic = np.asarray(ic_vec, dtype=np.float64)
    mu, sd = np.nanmean(ic), np.nanstd(ic) + cfg.get("std_eps", 1e-6)
    z = (ic - mu) / sd
    w = _softmax_vec(z, temp=cfg.get("softmax_temp", 1.0))
    w = np.maximum(w, cfg.get("min_weight", 0.0)); w = w / w.sum()
    return w

# NEW: EWMA smoother for monthly ICs (half-life in months)
def _ic_ewma(ic_list, halflife_m: float) -> float:
    arr = np.asarray([x for x in ic_list if np.isfinite(x)], dtype=np.float64)
    if arr.size == 0: return np.nan
    lam = 0.5**(1.0 / max(1.0, float(halflife_m)))
    # assume ic_list is chronological (oldest -> newest)
    weights = np.array([lam**(arr.size-1 - i) for i in range(arr.size)], dtype=np.float64)
    return float(np.sum(weights * arr) / max(1e-12, np.sum(weights)))


def _weighted_quantile(values_mat, weights, q):
    L, N = values_mat.shape
    order = np.argsort(values_mat, axis=0)
    w_sorted = np.take(weights, order)
    v_sorted = np.take_along_axis(values_mat, order, axis=0)
    cw = np.cumsum(w_sorted, axis=0)
    idx = (cw >= q - 1e-12).argmax(axis=0)
    idx = np.clip(idx, 0, L-1)
    return v_sorted[idx, np.arange(N)]

# ======== Walk-forward ========
_p(f"[WF] Start: months={len(dates_master)}, features={X_all.shape[1]}, horizons={CFG3['horizons']}")
_p(f"Score families → normal={CFG3['scoring_map']['normal']} | stress={CFG3['scoring_map']['stress']}")

for it, dt in enumerate(dates_master):
    try:
        # roll forward carry state (decision & booked)
        if dt in ret_m.index:
            r_dt = ret_m.loc[dt].reindex(w_prev_state_dec.index).fillna(0.0).astype(float)
            denom_dec = 1.0 + float((w_prev_state_dec * r_dt).sum()); denom_dec = 1.0 if (not np.isfinite(denom_dec) or denom_dec <= 1e-12) else denom_dec
            w_pre_dec = (w_prev_state_dec * (1.0 + r_dt)) / denom_dec
            denom_book = 1.0 + float((w_prev_state_book * r_dt).sum()); denom_book = 1.0 if (not np.isfinite(denom_book) or denom_book <= 1e-12) else denom_book
            w_pre_book = (w_prev_state_book * (1.0 + r_dt)) / denom_book
        else:
            w_pre_dec  = w_prev_state_dec.copy(); w_pre_book = w_prev_state_book.copy()

        end_train  = dt - pd.offsets.MonthEnd(1)
        start_train= end_train - pd.DateOffset(months=C("windows.train_months") + C("windows.valid_months") - 1)
        calib_start= end_train - pd.DateOffset(months=C("windows.calib_months") - 1)

        X_tr = panel_slice(X_all, start_train, end_train)
        if X_tr.empty:
            _decision_carry(dt, w_pre_dec, dec_last_regime, "carry_drift"); _carry_no_trade(dt, w_pre_book, dec_last_regime, "carry_drift"); continue

        months_span = sorted(X_tr.index.get_level_values(0).unique())
        if len(months_span) < (C("windows.valid_months") + 1):
            _decision_carry(dt, w_pre_dec, dec_last_regime, "carry_drift"); _carry_no_trade(dt, w_pre_book, dec_last_regime, "carry_drift"); continue

        if C("risk.require_vol_target_ready", True):
            i_dt_cov = (ret_m.index.get_loc(dt) if dt in ret_m.index else -1)
            if i_dt_cov < int(C("risk.cov_lookback_m")):
                _decision_carry(dt, w_pre_dec, dec_last_regime, "warm-up_no_trade"); _carry_no_trade(dt, w_pre_book, dec_last_regime, "warm-up_no_trade"); continue

        valid_months = months_span[-C("windows.valid_months"):]
        Ys = {h: targets_h[h].reindex(X_tr.index) for h in CFG3["horizons"]}

        idx_keep = X_tr.index
        for h in CFG3["horizons"]:
            dt_cut_h = end_train - pd.DateOffset(months=h)
            idx_h = Ys[h].dropna().index
            mask_h = idx_h.get_level_values(0) <= dt_cut_h
            idx_keep = idx_keep.intersection(idx_h[mask_h])
        if len(idx_keep) == 0:
            _decision_carry(dt, w_pre_dec, dec_last_regime, "no_labels"); _carry_no_trade(dt, w_pre_book, dec_last_regime, "no_labels"); continue

        X_tr = X_tr.loc[idx_keep].astype(np.float32, copy=False)
        mvals = X_tr.index.get_level_values(0)
        mask_valid = pd.Index(mvals).isin(valid_months)
        mask_train = ~mask_valid
        X_arr = X_tr.values.astype(np.float32, copy=False)
        params = C("lgb.base_params").copy()
        early_cp = C("lgb.early_stop_checkpoints")
        K_ic = int(C("portfolio.names_target"))

        beta_mi = beta_ts.stack()
        y_plain, y_resid = {}, {}
        for h in CFG3["horizons"]:
            y_raw = Ys[h].loc[idx_keep]["y"].astype(np.float32).values
            y_plain[h] = pd.Series(y_raw, index=idx_keep, dtype=np.float32)
            mkt_h = mkt_h_ret[h].reindex(idx_keep.get_level_values(0)).astype(np.float32).values
            beta_aligned = beta_mi.reindex(idx_keep).astype(np.float32).values
            y_resid[h] = pd.Series(y_raw - beta_aligned * mkt_h, index=idx_keep, dtype=np.float32)

        # ==========================
        # ENSEMBLE-AWARE _train_stack (per-iteration to use local masks) — CHANGED
        # ==========================
        def _train_stack(Y_map):
            ens_cfg = C("ensemble", {"enable": False}); use_ens = bool(ens_cfg.get("enable", False))
            base_params = C("lgb.base_params").copy(); early_cp = C("lgb.early_stop_checkpoints")
            K_ic = int(C("portfolio.names_target")); wcfg = ens_cfg.get("weighting", {})
            learners = ens_cfg.get("learners", [{"name":"base"}]) if use_ens else [{"name":"base"}]
            H = list(CFG3["horizons"])
            models_mean = {h: [] for h in H}; chosen_rounds = {h: [] for h in H}
            ic_valid_all = []; per_learner_ic = {h: [] for h in H}; q_models = {h: [] for h in H}

            for Lrn in learners:
                lp = base_params.copy()
                for k, v in Lrn.items():
                    if k == "name": continue
                    elif k in ("seed", "random_state"): lp["seed"] = int(v)
                    elif k == "bagging_seed": lp["bagging_seed"] = int(v)
                    elif k == "feature_fraction_seed": lp["feature_fraction_seed"] = int(v)
                    else: lp[k] = v

                # collect monthly IC series for EWMA weighting
                ic_series_by_h = {h: [] for h in H}

                for h in H:
                    y_arr = Y_map[h].values.astype(np.float32, copy=False)
                    model = fit_lgb_reg(X_arr[mask_train], y_arr[mask_train], lp, max(early_cp))
                    idx_v  = X_tr.index[mask_valid]; m_v = idx_v.get_level_values(0); isin_v = idx_v.get_level_values(1)
                    best_ic, best_n = -1e9, early_cp[0]; best_monthly_ics=[]
                    for n_boost in early_cp:
                        pred_v = model.predict(X_arr[mask_valid], num_iteration=n_boost)
                        df_v = pd.DataFrame({"Date": m_v.values, "ISIN": isin_v.values, "pred": pred_v, "y": Y_map[h].values[mask_valid]})
                        ics=[]
                        for mdt in sorted(df_v["Date"].unique()):
                            dfm = df_v[df_v["Date"] == mdt]
                            if len(dfm) < max(10, K_ic): continue
                            kth = dfm["pred"].nlargest(K_ic).min()
                            dfmK = dfm[dfm["pred"] >= kth]
                            ic = rank_ic_tieaware(dfmK["pred"].values, dfmK["y"].values, ids=dfmK["ISIN"].values, eps=1e-12)
                            if np.isfinite(ic): ics.append(ic)
                        if len(ics) >= 4:
                            mean_ic = float(np.mean(ics))
                            if np.isfinite(mean_ic) and mean_ic > best_ic:
                                best_ic, best_n = mean_ic, n_boost; best_monthly_ics = ics[:]
                    models_mean[h].append(model); chosen_rounds[h].append(best_n)
                    if len(best_monthly_ics): ic_valid_all += best_monthly_ics
                    # CHANGED: store full per-month IC series (chronological) for EWMA weighting
                    ic_series_by_h[h] = best_monthly_ics[:] if len(best_monthly_ics) else []

                # CHANGED: after training all horizons for this learner, compute EWMA IC per horizon
                hl = float(wcfg.get("ewma_halflife_m", 6))
                for h in H:
                    if len(ic_series_by_h[h]) >= 1:
                        ewma_ic = _ic_ewma(ic_series_by_h[h], hl)
                        per_learner_ic[h].append(ewma_ic if np.isfinite(ewma_ic) else -1e9)
                    else:
                        per_learner_ic[h].append(-1e9)

                for h in H:
                    y_arr = Y_map[h].values.astype(np.float32, copy=False); qdict = {}
                    for q in C("uncertainty.quantiles"): qdict[q] = fit_lgb_quantile(X_arr[mask_train], y_arr[mask_train], lp, q, chosen_rounds[h][-1])
                    q_models[h].append(qdict)

            ic_stats = {"ic_mean": (float(np.mean(ic_valid_all)) if len(ic_valid_all) else np.nan),
                        "ic_std":  (float(np.std(ic_valid_all, ddof=1)) if len(ic_valid_all)>1 else np.nan)}

            X_cal = panel_slice(X_all, calib_start, end_train).astype(np.float32, copy=False); idx_cal = X_cal.index
            w_per_h = {}
            for h in H:
                w_per_h[h] = np.ones(len(learners), dtype=np.float64)
                if use_ens and len(learners) > 1: w_per_h[h] = _ic_weights(per_learner_ic[h], wcfg)

            conformal_add = {}
            for h in H:
                dt_cut_cal = end_train - pd.DateOffset(months=h)
                idx_cal_h = idx_cal.intersection(Y_map[h].index)
                mask_cal = idx_cal_h.get_level_values(0) <= dt_cut_cal
                idx_cal_h2 = idx_cal_h[mask_cal]
                if len(idx_cal_h2) < 1000: conformal_add[h] = (0.0, 0.0); continue
                Xc = X_all.loc[idx_cal_h2].values.astype(np.float32, copy=False)
                yc = Y_map[h].loc[idx_cal_h2].values.astype(np.float32, copy=False)
                preds_mat = []
                for j, model in enumerate(models_mean[h]):
                    pj = model.predict(Xc, num_iteration=chosen_rounds[h][j]); preds_mat.append(pj.astype(np.float64, copy=False))
                preds_mat = np.vstack(preds_mat)
                _ = np.average(preds_mat, axis=0, weights=w_per_h[h])  # ensemble mean (unused here)
                qlo_mat, qhi_mat = [], []
                for j in range(len(learners)):
                    qlo_mat.append(q_models[h][j][C("uncertainty.quantiles")[0]].predict(Xc, num_iteration=chosen_rounds[h][j]))
                    qhi_mat.append(q_models[h][j][C("uncertainty.quantiles")[1]].predict(Xc, num_iteration=chosen_rounds[h][j]))
                qlo_mat = np.vstack(qlo_mat); qhi_mat = np.vstack(qhi_mat)
                vinc_q = float(C("ensemble.quantile.vinc_q", 0.50))
                if C("ensemble.quantile.use_vincentization", True) and len(learners) > 1:
                    qlo_ens = _weighted_quantile(qlo_mat, w_per_h[h], vinc_q)
                    qhi_ens = _weighted_quantile(qhi_mat, w_per_h[h], 1.0 - vinc_q)
                else:
                    qlo_ens = np.average(qlo_mat, axis=0, weights=w_per_h[h])
                    qhi_ens = np.average(qhi_mat, axis=0, weights=w_per_h[h])

                res_low  = np.maximum(0.0, (qlo_ens - yc)); res_high = np.maximum(0.0, (yc - qhi_ens))
                add_low  = np.quantile(res_low,  1.0 - C("uncertainty.conformal_alpha")) if len(res_low)  else 0.0
                add_high = np.quantile(res_high, 1.0 - C("uncertainty.conformal_alpha")) if len(res_high) else 0.0
                conformal_add[h] = (float(add_low), float(add_high))

            iso = None
            h_q = min(H)
            dt_cut_iso = end_train - pd.DateOffset(months=h_q)
            idx_cal_h1 = idx_cal.intersection(Y_map[h_q].index)
            mask_iso = idx_cal_h1.get_level_values(0) <= dt_cut_iso
            idx_cal_h1 = idx_cal_h1[mask_iso]
            if len(idx_cal_h1) >= 200:
                Xc = X_all.loc[idx_cal_h1].values.astype(np.float32, copy=False)
                yc = Y_map[h_q].loc[idx_cal_h1].values.astype(np.float32, copy=False)
                pm = []
                for j, model in enumerate(models_mean[h_q]):
                    pj = model.predict(Xc, num_iteration=chosen_rounds[h_q][j]); pm.append(pj.astype(np.float64, copy=False))
                pm = np.vstack(pm); yhat_cal = np.average(pm, axis=0, weights=w_per_h[h_q])
                iso = IsotonicRegression(out_of_bounds="clip").fit(yhat_cal, yc)

            def _predict_ensemble(X_now_mat, names_idx):
                mu_h, qlo_h, qhi_h = {}, {}, {}
                for h in H:
                    pm = []
                    for j, model in enumerate(models_mean[h]):
                        pm.append(model.predict(X_now_mat, num_iteration=chosen_rounds[h][j]))
                    pm = np.vstack(pm)
                    mu = np.average(pm, axis=0, weights=w_per_h[h])
                    if (h == min(H)) and (iso is not None): mu = iso.predict(mu)
                    qlo_list, qhi_list = [], []
                    for j in range(len(learners)):
                        qlo_list.append(q_models[h][j][C("uncertainty.quantiles")[0]].predict(X_now_mat, num_iteration=chosen_rounds[h][j]))
                        qhi_list.append(q_models[h][j][C("uncertainty.quantiles")[1]].predict(X_now_mat, num_iteration=chosen_rounds[h][j]))
                    qlo_mat = np.vstack(qlo_list); qhi_mat = np.vstack(qhi_list)
                    vinc_q = float(C("ensemble.quantile.vinc_q", 0.50))
                    if C("ensemble.quantile.use_vincentization", True) and len(learners) > 1:
                        qlo = _weighted_quantile(qlo_mat, w_per_h[h], vinc_q)
                        qhi = _weighted_quantile(qhi_mat, w_per_h[h], 1.0 - vinc_q)
                    else:
                        qlo = np.average(qlo_mat, axis=0, weights=w_per_h[h])
                        qhi = np.average(qhi_mat, axis=0, weights=w_per_h[h])

                    add_low, add_high = conformal_add[h]
                    qlo_h[h] = pd.Series(qlo - add_low, index=names_idx)
                    qhi_h[h] = pd.Series(qhi + add_high, index=names_idx)
                    mu_h[h]  = pd.Series(mu, index=names_idx)
                return mu_h, qlo_h, qhi_h
            return _predict_ensemble, ic_stats, iso
        # ==========================

        predict_plain, ic_plain, iso_plain = _train_stack(y_plain)
        predict_resid, ic_resid, iso_resid = _train_stack(y_resid)

        # Universe & blacklist
        names = month_constituents(dt, ret_m.columns)
        _bl = set(map(str, C("universe.blacklist", [])))
        if _bl: names = [n for n in names if str(n) not in _bl]
        if len(names) < 1:
            _decision_carry(dt, w_pre_dec, dec_last_regime, "no_names"); _carry_no_trade(dt, w_pre_book, dec_last_regime, "no_names"); continue

        idx_now = pd.MultiIndex.from_product([[dt], names], names=["Date","ISIN"])
        X_now_df = X_all.reindex(idx_now).astype(np.float32, copy=False)
        if X_now_df is None or X_now_df.empty:
            _decision_carry(dt, w_pre_dec, dec_last_regime, "no_features_now"); _carry_no_trade(dt, w_pre_book, dec_last_regime, "no_features_now"); continue
        X_now = X_now_df.values.astype(np.float32, copy=False); names_idx = X_now_df.index.get_level_values(1)

        mu_plain, qlo_plain, qhi_plain = predict_plain(X_now, names_idx)
        mu_resid, qlo_resid, qhi_resid = predict_resid(X_now, names_idx)

        pred_log[dt] = pd.DataFrame({"ISIN": mu_resid[min(CFG3["horizons"])].index.astype(str),
                                     "mu_hat_resid": mu_resid[min(CFG3["horizons"])].values})

        cooldown_applied = False; pre_stage_forced = False
        ml_cfg = C("regimes.ml", {}); ml_enabled = bool(ml_cfg.get("enable", False))
        p_stress, det_iso_flag, det_ready, det_counts = (np.nan, "off", False, {"pos":0,"neg":0,"obs":0})
        if ml_enabled:
            p_stress, det_iso_flag, det_ready, det_counts = _train_and_score_stress_clf(end_train_dt=end_train, score_dt=dt)

        enter_n = int(C("regimes.hysteresis.enter_need_consecutive", 1))
        exit_n  = int(C("regimes.hysteresis.exit_need_consecutive", 1))
        rule_abs_now, vol_prev, r_prev, vol_th_eff, drop_th_eff, _ = is_risk_off_month(dt)
        emergency_triggered = bool(rule_abs_now)

        if det_ready and np.isfinite(p_stress):
            tau_in  = float(ml_cfg.get("tau_in", 0.60)); tau_out = float(ml_cfg.get("tau_out", 0.45))
            if dec_last_regime != "stress":
                if p_stress >= tau_in: risk_off_streak += 1; risk_on_streak = 0
                else: risk_off_streak = 0; risk_on_streak = 0
                risk_off = (risk_off_streak >= enter_n)
            else:
                if p_stress <= tau_out: risk_on_streak  += 1; risk_off_streak = 0
                else: risk_on_streak = 0
                risk_off = not (risk_on_streak >= exit_n)
            if emergency_triggered and dec_last_regime != "stress":
                risk_off = True; risk_off_streak = max(risk_off_streak, enter_n); risk_on_streak = 0
        else:
            if emergency_triggered: risk_off_streak += 1; risk_on_streak = 0
            else: risk_on_streak  += 1; risk_off_streak = 0
            if dec_last_regime != "stress": risk_off = (risk_off_streak >= enter_n)
            else: risk_off = not (risk_on_streak >= exit_n)

        strong_factors, _ovd = _factor_override_strength(mu_plain, qlo_plain, qhi_plain, ic_plain.get("ic_mean", np.nan))

        if risk_off:
            if C("prestress.force_before_stress", False) and (dec_last_regime not in ("pre-stress","stress")):
                regime_candidate = "pre-stress"; pre_stage_forced = True
            else: regime_candidate = "stress"
        else:
            if C("prestress.force_after_stress", False) and (dec_last_regime == "stress"):
                regime_candidate = "pre-stress"; cooldown_applied = True
            else:
                streak = 0
                if shadow_dates_traded:
                    th = float(C("prestress.underperf_threshold"))
                    idx = pd.DatetimeIndex(shadow_dates_traded); rg  = pd.Series(shadow_ret_gross_m, index=idx)
                    bench = mkt_ret_m.reindex(idx).fillna(0.0); excess = (rg - bench).astype(float)
                    for d in sorted(idx[idx < dt], reverse=True):
                        if str(shadow_regime_at_trade_date.get(d, "")).lower() != "normal": break
                        if float(excess.get(d, 0.0)) <= th + 1e-12: streak += 1
                        else: break
                if (streak >= int(C("prestress.consecutive_enter"))) and (not strong_factors):
                    regime_candidate = "pre-stress"
                else: regime_candidate = "normal"

        regime_kind = regime_candidate
        if dec_last_regime == "stress" and stress_dwell_left > 0: regime_kind = "stress"
        elif dec_last_regime == "pre-stress" and prestress_dwell_left > 0: regime_kind = "pre-stress"

        names_idx = X_now_df.index.get_level_values(1)
        scores_normal_like, meth_normal = _build_family_scores(dt, names_idx, mu_plain, qlo_plain, qhi_plain,
                                                               mu_resid, qlo_resid, qhi_resid, family="normal")
        scores_stress_like, meth_stress = _build_family_scores(dt, names_idx, mu_plain, qlo_plain, qhi_plain,
                                                               mu_resid, qlo_resid, qhi_resid, family="stress")
        note_method = ""
        if regime_kind == "normal":
            scores = scores_normal_like; note_method = f"{meth_normal}"
        elif regime_kind == "stress":
            scores = scores_stress_like; note_method = f"{meth_stress}"
        else:
            bcfg = C("prestress.blend")
            lam_base = float(bcfg["lam_hi_cooldown"] if cooldown_applied or pre_stage_forced else bcfg["lam_lo_deterior"])
            if strong_factors and (not bcfg["nudge_requires_vol_below_thresh"] or (not np.isfinite(vol_prev) or vol_prev < vol_th_eff)):
                lam_base = min(bcfg["lam_hi_cooldown"], lam_base + float(bcfg["factor_nudge"]))
            if np.isfinite(vol_prev) and np.isfinite(vol_th_eff) and (vol_prev >= vol_th_eff):
                lam_base = min(lam_base, float(bcfg["vol_high_clamp"]))
            step = float(bcfg["lam_step_max"]); lo, hi = map(float, bcfg["lam_bounds"])
            lam = float(np.clip(dec_prestress_lambda_prev + np.clip(lam_base - dec_prestress_lambda_prev, -step, step), lo, hi))
            dec_prestress_lambda_prev = lam
            scores = (1.0 - lam) * scores_stress_like + lam * scores_normal_like
            note_method = f"blend(lam={lam:.2f},N={meth_normal},S={meth_stress})"

        use_stress_band = (regime_kind=="stress" and C("regimes.stress.breadth.enable"))
        if use_stress_band:
            names_cap = int(C("regimes.stress.breadth.names_max")); stress_min = int(C("regimes.stress.breadth.names_min"))
            names_limit_max = int(C("regimes.stress.breadth.names_max"))
        else:
            names_cap = int(C("portfolio.names_target")); stress_min = None
            names_limit_max = int(C("portfolio.names_max"))

        fallback_used = False; used_regime = regime_kind

        # ===== Allocation attempt (with global cap enforcement + breadth relaxation) =====
        def _alloc_try(regime_for_try: str):
            # choose score series
            if regime_for_try == "normal": sc = scores_normal_like; sm = meth_normal; use_resid = False
            elif regime_for_try == "stress": sc = scores_stress_like; sm = meth_stress; use_resid = True
            else:
                bcfg = C("prestress.blend"); lam_tmp = dec_prestress_lambda_prev
                sc = (1.0 - lam_tmp)*scores_stress_like + lam_tmp*scores_normal_like
                sm = f"blend(lam={lam_tmp:.2f},N={meth_normal},S={meth_stress})"; use_resid = True
            if sc.sum() == 0: return {"status":"fail","fail_reason":"no_scores"}

            top_all = sc.sort_values(ascending=False)
            # initial breadth
            k_init = names_cap
            if use_stress_band and stress_min is not None and k_init < stress_min:
                k_init = stress_min
            k_init = min(k_init, len(top_all))
            if k_init < 2: return {"status":"fail","fail_reason":"too_few_names"}

            # helper to build ERC for a given top set size
            def _alloc_for_k(k_use: int):
                top = top_all.head(k_use)

                i_dt_cov = ret_m.index.get_loc(dt); look = int(C("risk.cov_lookback_m"))
                hist = ret_m.iloc[i_dt_cov-look:i_dt_cov].reindex(columns=top.index).fillna(0.0)
                if hist.shape[0] < max(24, look) or hist.shape[1] < 2:
                    return None

                # base covariance -> shrink
                S = np.cov(hist.values, rowvar=False)
                Sigma_base = _shrink_cov(S, float(C("risk.cov_shrink_rho")))

                # PCA condition
                pca_cfg = C("risk_pca", {})
                Sigma_pca, U, lam = _condition_cov_pca(
                    Sigma_base,
                    var_explain_target=float(pca_cfg.get("var_explain_target", 0.85)),
                    eig_cap_ratio=float(pca_cfg.get("eig_cap_ratio", 10.0)),
                    eig_floor_eps=float(pca_cfg.get("eig_floor_eps", 1e-8))
                )

                # liquidity diagonal inflation
                adv_series = _adv_asof_wrapper(dt, top.index, _ADV_LOOK_eff, _ADV_FLOOR_eff)
                liq_cfg = pca_cfg.get("liq_diag", {"gamma":1.0,"lambda":1.0})
                Sigma = _liquidity_diagonal_inflate(
                    Sigma_pca, adv_series.reindex(top.index),
                    gamma=float(liq_cfg.get("gamma", 1.0)),
                    power=float(liq_cfg.get("lambda", 1.0)),
                    adv_floor=_ADV_FLOOR_eff
                )

                # per-name caps (absolute, AUM fraction)
                w_cap_adv_abs = (_ADV_FR_pos * adv_series / max(1e-12, _AUM_RS)).reindex(top.index).clip(lower=0.0).astype(float)
                erc_base = C("allocation.erc").copy()
                if regime_for_try == "pre-stress":
                    tmp = C("allocation.erc_prestress"); erc_base["rb_gamma"] = tmp["rb_gamma"]; erc_base["w_max_pct"] = tmp["w_max_pct"]
                if regime_for_try == "stress":
                    tmp = C("allocation.erc_stress"); erc_base["rb_gamma"] = tmp["rb_gamma"]; erc_base["w_max_pct"] = tmp["w_max_pct"]

                w_abs_global = (erc_base["w_max_pct"] / 100.0) * float(base_gross_min)
                w_cap_vec_abs_eff = np.minimum(w_cap_adv_abs, w_abs_global)

                # ERC with effective vector caps AND scalar cap enforced inside
                w_erc = erc_from_scores(
                    Sigma=Sigma, scores=top, gross_req=float(base_gross_min),
                    w_floor_abs=float(w_floor), w_max_abs=float(erc_base["w_max_pct"]/100.0),
                    gamma=float(erc_base["rb_gamma"]), eps=float(C("allocation.erc.rb_eps")),
                    max_iter=int(C("allocation.erc.max_iter")), tol=float(C("allocation.erc.tol")),
                    step=float(C("allocation.erc.step")), verbose=bool(C("allocation.erc.verbose")),
                    w_max_vec_abs=w_cap_vec_abs_eff
                )

                w_new_full = pd.Series(0.0, index=ret_m.columns)
                if len(w_erc): w_new_full.loc[w_erc.index] = w_erc.values

                # PC1 risk contribution cap (nudge) — reproject to effective caps
                try:
                    w_top = w_new_full.loc[top.index].values
                    w_nudged, pc1_share = _pc1_share_and_nudge(
                        w_top, Sigma, U, lam,
                        cap=float(pca_cfg.get("post_nudge", {}).get("pc1_rc_cap", 0.40)),
                        trust_radius=float(pca_cfg.get("post_nudge", {}).get("trust_radius_l1", 0.10)),
                        iters=int(pca_cfg.get("post_nudge", {}).get("iters", 2))
                    )
                    if np.isfinite(pc1_share) and (w_nudged is not None):
                        lower = np.zeros_like(w_nudged)
                        upper = w_cap_vec_abs_eff.reindex(top.index).to_numpy(np.float64)
                        b = np.maximum(1e-12, top.values.astype(np.float64))
                        w_nudged = _project_box_sum(w_nudged, float(base_gross_min), lower, upper, b, max_iter=50)
                        w_new_full.loc[top.index] = w_nudged
                except Exception:
                    pass

                # trade caps (per-month change) from ADV/AUM — projected with global cap
                delta_cap_abs = (_trade_FR * adv_series / max(1e-12, _AUM_RS)).reindex(top.index).clip(lower=0.0).astype(float)
                w_pre_top = w_pre_dec.reindex(top.index).fillna(0.0).clip(lower=0.0).to_numpy(np.float64)
                lower = np.maximum(0.0, w_pre_top - delta_cap_abs.to_numpy(np.float64))
                upper = np.minimum(
                    w_cap_vec_abs_eff.to_numpy(np.float64),
                    w_pre_top + delta_cap_abs.to_numpy(np.float64)
                )

                w_top_now = w_new_full.loc[top.index].to_numpy(np.float64)
                b = np.maximum(1e-12, top.values.astype(np.float64))

                # feasible target sum and capacity window
                sum_upper = float(np.sum(upper))
                target_sum = float(min(base_gross_min, sum_upper))
                target_sum = float(max(target_sum, float(np.sum(lower))))

                w_top_proj = _project_box_sum(w_top_now, target_sum, lower, upper, b, max_iter=50)
                w_new_full.loc[top.index] = w_top_proj

                # readiness & vol estimate
                vol_ready, reason = _vol_ready_for(dt, w_new_full)
                if not vol_ready:
                    return {"status":"fail","fail_reason":f"vol_not_ready({reason})"}

                wv = w_new_full.loc[top.index].values
                vol_ann_est = _vol_ann_of(wv, Sigma)

                # NEW: alpha proxy to prefer high-alpha breadth when relaxing
                alpha_proxy = float((top * w_new_full.loc[top.index]).sum())

                return {
                    "status": "ok",
                    "w_full": w_new_full,
                    "Sigma": Sigma,
                    "top_idx": top.index,
                    "sum_upper": sum_upper,
                    "vol_ann_est": vol_ann_est,
                    "w_abs_global": w_abs_global,
                    "caps_eff": w_cap_vec_abs_eff,
                    "scores_method": sm,
                    "alpha_proxy": alpha_proxy
                }

            # initial allocation candidate list
            first = _alloc_for_k(k_init)
            if first is None or first.get("status") != "ok":
                return {"status":"fail","fail_reason":"no_trade_erc_hist_short"}

            # Evaluate breadths; choose by vol-shortfall then alpha-proxy, while meeting capacity
            vtarget_now = _dyn_vol_target(dt, regime_for_try)
            tol_vol = float(C("allocation.vol_underutil_tol", 0.02))
            cands = [first]
            if k_init < names_limit_max:
                step = int(C("allocation.breadth_step", 5))
                for k in range(min(k_init + step, names_limit_max), min(len(top_all), names_limit_max) + 1, step):
                    cand = _alloc_for_k(k)
                    if cand is not None and cand.get("status") == "ok":
                        cands.append(cand)

            # choose best candidate
            def _vol_shortfall(c): return max(0.0, vtarget_now - float(c["vol_ann_est"]))
            meet_cap = [c for c in cands if c["sum_upper"] >= base_gross_min - 1e-9]

            if meet_cap:
                # prefer minimal shortfall to target; tie-break by higher alpha
                best = min(meet_cap, key=lambda c: (_vol_shortfall(c), -c["alpha_proxy"]))
                # if vol is within tolerance, fine; otherwise best we can do under caps
                pass
            else:
                # capacity not met anywhere: take largest achievable sum_upper, tie-break by alpha
                best = max(cands, key=lambda c: (c["sum_upper"], c["alpha_proxy"]))

            # beta backstop / mix to target (only after breadth relaxation)
            w_new_full = best["w_full"].copy()
            Sigma = best["Sigma"]; top = best["top_idx"]; vol_ann_est = best["vol_ann_est"]

            if regime_for_try in ("normal","pre-stress"):
                w_new_full, vol_ann_est, beta_note = _maybe_beta_fill(
                    regime_kind=regime_for_try, dt=dt, names_idx=names_idx, top_idx=top,
                    Sigma=Sigma, X_now_df=X_now_df, w_new_full=w_new_full.copy(),
                    vtarget_now=vtarget_now, gross_req=base_gross_min, vol_ann_est=vol_ann_est
                )
            else:
                bb = C("regimes.stress.beta_backstop", {})
                if bb and bb.get("enable", False):
                    try:
                        min_rel_gap = float(bb.get("min_rel_gap", 0.20))
                        gap_ok = (vtarget_now - vol_ann_est) > (min_rel_gap * max(vol_ann_est, 1e-12))
                        mom_h = int(bb.get("bench_mom_h", 3))
                        bench_hist = mkt_ret_m.reindex(ret_m.index[ret_m.index < dt]).tail(mom_h).astype(float)
                        bench_mom = float((1.0 + bench_hist.fillna(0.0)).prod() - 1.0)
                        if gap_ok and np.isfinite(bench_mom) and bench_mom > 0.0:
                            betas_t = beta_ts.loc[dt] if dt in beta_ts.index else pd.Series(C("scoring.fallbacks.beta"), index=ret_m.columns)
                            bet = betas_t.reindex(top).clip(lower=0.0).replace([np.inf,-np.inf,np.nan], 0.0)
                            s = float(bet.sum()); w_beta = (bet / (s if s>0 else 1.0)) * float(base_gross_min)
                            t_cap = float(bb.get("max_mix", 0.25))
                            w_alpha = w_new_full.loc[top].values; w_mix = (1.0 - t_cap)*w_alpha + t_cap*w_beta.values
                            vol_mix = _vol_ann_of(w_mix, Sigma)
                            if vol_mix > vol_ann_est + 1e-12:
                                w_new_full.loc[top] = w_mix; vol_ann_est = vol_mix; sm = sm + "|beta_backstop"
                    except Exception: pass

            iso_flag = "on" if ((regime_for_try in ("pre-stress","stress") and (iso_resid is not None)) or
                                (regime_for_try=="normal" and (iso_plain is not None))) else "off"

            return {"status":"ok","w": w_new_full,"vol_ann": vol_ann_est,
                    "scores_method": best["scores_method"],
                    "iso_flag": iso_flag, "note_extra":"", "top_idx": best["top_idx"]}

        # Try primary regime allocation
        res = _alloc_try(used_regime)

        # Fallback chain if needed
        if res.get("status") != "ok" and res.get("fail_reason") == "no_scores":
            if used_regime == "stress": chain = ["pre-stress", "safe"]
            elif used_regime == "pre-stress": chain = ["stress", "safe"]
            else: chain = ["pre-stress", "safe"]
            for step_kind in chain:
                if step_kind == "safe":
                    res_fb = _safe_beta_equal_weight(dt, names_idx, from_regime=used_regime)
                    if res_fb.get("status") == "ok": res = res_fb; fallback_used = True; break
                else:
                    res_fb = _alloc_try(step_kind)
                    if res_fb.get("status") == "ok":
                        res = res_fb; used_regime = step_kind
                        res["note_extra"] = f"fallback={step_kind.replace('-','_')}_from_{regime_kind.replace('-','_')}"
                        fallback_used = True; break
                    elif res_fb.get("fail_reason") != "no_scores": break

        # If still failed, try safe-stress ladder; else carry
        if res.get("status") != "ok":
            reason = res.get("fail_reason","alloc_failed")
            if used_regime == "stress":
                w_safe, vol_safe, why = _build_safe_stress_weights(dt, names_idx, base_gross_min, w_pre_dec)
                if w_safe is not None and w_safe.sum() > 0:
                    prev_reg = dec_last_regime
                    streak_for_note_dec = 0
                    if shadow_dates_traded:
                        th = float(C("prestress.underperf_threshold"))
                        idx = pd.DatetimeIndex(shadow_dates_traded); rg  = pd.Series(shadow_ret_gross_m, index=idx)
                        bench = mkt_ret_m.reindex(idx).fillna(0.0); excess = (rg - bench).astype(float)
                        for d in sorted(idx[idx < dt], reverse=True):
                            if str(shadow_regime_at_trade_date.get(d, "")).lower() != "normal": break
                            if float(excess.get(d, 0.0)) <= th + 1e-12: streak_for_note_dec += 1
                            else: break
                    note_txt_dec = f"score=safe_stress({why})|streak={streak_for_note_dec}|cooldown={int(cooldown_applied)}|preforce={int(pre_stage_forced)}|iso=n/a|emg_abs={int(emergency_triggered)}"
                    _sim_next_month(dt, w_pre_dec, w_safe, used_regime, note_txt_dec)
                    dec_last_regime = used_regime; _update_dwell_counters(prev_reg, dec_last_regime)
                    if (not live_started) and C("gating.require_iso_before_live", True):
                        _p(f"{dt:%Y-%m}: pre-live gating — ISO not ready (safe_stress path). Skipping booking."); continue
                    if (not live_started) and ml_enabled and bool(ml_cfg.get("require_to_go_live", True)) and (not det_ready):
                        _p(f"{dt:%Y-%m}: pre-live gating — detector not ready (safe_stress path; obs={det_counts['obs']}, pos={det_counts['pos']}, neg={det_counts['neg']}). Skipping booking."); continue
                    if not live_started: live_started = True
                    weights_by_date[dt] = w_safe[w_safe>0].copy()
                    streak_for_note = streak_for_note_dec
                    note_txt = f"score=safe_stress({why})|streak={streak_for_note}|cooldown={int(cooldown_applied)}|preforce={int(pre_stage_forced)}|iso=n/a|emg_abs={int(emergency_triggered)}"
                    _book_next_month(dt, w_pre_book, w_safe, used_regime, vol_safe, note=note_txt)
                    last_live_regime = used_regime; w_prev_state_book = w_safe.copy(); continue
            _decision_carry(dt, w_pre_dec, dec_last_regime, f"no_trade_{used_regime}({reason})")
            _carry_no_trade(dt, w_pre_book, dec_last_regime, f"no_trade_{used_regime}({reason})"); continue

        # Decision tape (shadow)
        w_new_full = res["w"]
        if w_new_full.sum() <= 0:
            _decision_carry(dt, w_pre_dec, dec_last_regime, "zero_weights"); _carry_no_trade(dt, w_pre_book, dec_last_regime, "zero_weights"); continue

        streak_for_note_dec = 0
        if shadow_dates_traded:
            th = float(C("prestress.underperf_threshold"))
            idx = pd.DatetimeIndex(shadow_dates_traded); rg  = pd.Series(shadow_ret_gross_m, index=idx)
            bench = mkt_ret_m.reindex(idx).fillna(0.0); excess = (rg - bench).astype(float)
            for d in sorted(idx[idx < dt], reverse=True):
                if str(shadow_regime_at_trade_date.get(d, "")).lower() != "normal": break
                if float(excess.get(d, 0.0)) <= th + 1e-12: streak_for_note_dec += 1
                else: break

        note_parts_dec = [
            f"score={res['scores_method']}",
            f"streak={streak_for_note_dec}",
            f"cooldown={int(cooldown_applied)}",
            f"preforce={int(pre_stage_forced)}",
            f"iso={'on' if (iso_resid is not None and used_regime!='normal') or (iso_plain is not None and used_regime=='normal') else 'off'}",
            f"det_p={ 'nan' if not np.isfinite(p_stress) else f'{p_stress:.2f}' }",
            f"det_iso={det_iso_flag}",
            f"det_ready={int(det_ready)}",
            f"emg_abs={int(emergency_triggered)}",
            f"det_obs={det_counts.get('obs',0)}",
            f"det_pos={det_counts.get('pos',0)}",
            f"det_neg={det_counts.get('neg',0)}"
        ]
        if fallback_used and res.get("note_extra"): note_parts_dec.append(res["note_extra"])
        note_txt_dec = "|".join(note_parts_dec)
        _sim_next_month(dt, w_pre_dec, w_new_full, used_regime, note_txt_dec)
        prev_reg = dec_last_regime; dec_last_regime = used_regime; _update_dwell_counters(prev_reg, dec_last_regime)

        # Live booking gates
        if (not live_started) and C("gating.require_iso_before_live", True) \
           and (str(res.get("iso_flag", "off")).lower() != "on"):
            _p(f"{dt:%Y-%m}: pre-live gating — ISO not ready (iso_flag={res.get('iso_flag')}). Skipping booking."); continue
        if (not live_started) and ml_enabled and bool(ml_cfg.get("require_to_go_live", True)) and (not det_ready):
            _p(f"{dt:%Y-%m}: pre-live gating — detector not ready (obs={det_counts.get('obs',0)}, pos={det_counts.get('pos',0)}, neg={det_counts.get('neg',0)}). Skipping booking."); continue

        if not live_started: live_started = True
        weights_by_date[dt] = w_new_full[w_new_full>0].copy()

        streak_for_note = streak_for_note_dec
        note_parts = [
            f"score={res['scores_method']}",
            f"streak={streak_for_note}",
            f"cooldown={int(cooldown_applied)}",
            f"preforce={int(pre_stage_forced)}",
            f"iso={'on' if (iso_resid is not None and used_regime!='normal') or (iso_plain is not None and used_regime=='normal') else 'off'}",
            f"det_p={ 'nan' if not np.isfinite(p_stress) else f'{p_stress:.2f}' }",
            f"det_iso={det_iso_flag}",
            f"det_ready={int(det_ready)}",
            f"emg_abs={int(emergency_triggered)}",
            f"det_obs={det_counts.get('obs',0)}",
            f"det_pos={det_counts.get('pos',0)}",
            f"det_neg={det_counts.get('neg',0)}"
        ]
        if fallback_used and res.get("note_extra"): note_parts.append(res["note_extra"])
        note_txt = "|".join(note_parts)
        _book_next_month(dt, w_pre_book, w_new_full, used_regime, res["vol_ann"], note=note_txt)
        last_live_regime = used_regime; w_prev_state_book = w_new_full.copy()

    except Exception as e:
        _p(f"❌ [WF ERROR] at dt={dt.date()} — {type(e).__name__}: {e}")
        raise

# --- Post-run: tapes & summary ---
idx = pd.DatetimeIndex(dates_traded)
ret_gross = pd.Series(ret_gross_m, index=idx)
ret_net   = pd.Series(ret_net_m,   index=idx)
turnover  = pd.Series(turnover_money_m, index=idx)
costs     = pd.Series(cost_m, index=idx)

def _compound_return(x: pd.Series) -> float:
    x = pd.Series(x).dropna().astype(float)
    return float((1.0 + x).prod() - 1.0) if len(x) else np.nan

if len(weights_by_date):
    inception_decision_dt = min(weights_by_date.keys())
    date_to_next = {dates_master[i]: dates_master[i+1] for i in range(len(dates_master)-1)}
    inception_ret_dt = date_to_next.get(inception_decision_dt, None)
else:
    inception_decision_dt = None; inception_ret_dt = None

if inception_ret_dt is not None:
    ret_gross_live = ret_gross[ret_gross.index >= inception_ret_dt].dropna()
    ret_net_live   = ret_net[ret_net.index >= inception_ret_dt].dropna()
    turnover_live  = turnover[turnover.index >= inception_ret_dt].dropna()
    costs_live     = costs[costs.index >= inception_ret_dt].dropna()
else:
    ret_gross_live = pd.Series(dtype=float); ret_net_live = pd.Series(dtype=float)
    turnover_live  = pd.Series(dtype=float); costs_live  = pd.Series(dtype=float)

cagr, vol_ann, mdd = ann_stats(ret_net_live)
turnover_ann = turnover_live.mean()*12.0 if len(turnover_live) else np.nan
cost_recov_ratio = (ret_gross_live.sum()/costs_live.sum()) if len(costs_live) and costs_live.sum()!=0 else np.nan
trailing_12m_ret = _compound_return(ret_net_live.iloc[-12:]) if len(ret_net_live)>=1 else np.nan

import os as _os; _os.makedirs("./portfolio_outputs", exist_ok=True)

rows=[]
for d in sorted(weights_by_date.keys())[-12:]:
    w = weights_by_date[d].sort_values(ascending=False)
    for isin, wi in w.items(): rows.append({"Date": d.strftime("%Y-%m-%d"), "ISIN": isin, "Weight(%)": round(100*wi,4)})
last12_path = _os.path.join("./portfolio_outputs", "portfolio_last_12m.csv")
pd.DataFrame(rows, columns=["Date","ISIN","Weight(%)"]).to_csv(last12_path, index=False)

w_rows=[]
for d in sorted(set(list(weights_pre_by_date.keys()) + list(weights_post_by_date.keys()))):
    wp = weights_pre_by_date.get(d, pd.Series(dtype=float)); wq = weights_post_by_date.get(d, pd.Series(dtype=float))
    names_out = sorted(set(wp.index.tolist()) | set(wq.index.tolist()))
    for n in names_out:
        w_rows.append({"Date": d.strftime("%Y-%m-%d"), "ISIN": n,
                       "Weight_pre(%)": round(100*float(wp.get(n, 0.0)), 6),
                       "Weight_post(%)": round(100*float(wq.get(n, 0.0)), 6)})
weights_tape_path = _os.path.join("./portfolio_outputs", "weights_tape.csv")
pd.DataFrame(w_rows, columns=["Date","ISIN","Weight_pre(%)","Weight_post(%)"]).to_csv(weights_tape_path, index=False)

o_rows=[]
for d in sorted(orders_by_date.keys()):
    dw = orders_by_date[d]
    if not isinstance(dw, pd.Series): continue
    for isin, delta in dw.items():
        if abs(float(delta)) < 1e-12: continue
        o_rows.append({"Date": d.strftime("%Y-%m-%d"), "ISIN": isin, "OrderDelta(%)": round(100*float(delta), 6)})
orders_tape_path = _os.path.join("./portfolio_outputs", "orders_tape.csv")
pd.DataFrame(o_rows, columns=["Date","ISIN","OrderDelta(%)"]).to_csv(orders_tape_path, index=False)

perf_tape_path = _os.path.join("./portfolio_outputs/performance_tape.csv")
if len(ret_net_live):
    perf = pd.DataFrame({
        "Ret_gross": ret_gross_live.astype(float),
        "Ret_net":   ret_net_live.astype(float),
        "Bench_ret": mkt_ret_m.reindex(ret_net_live.index).fillna(0.0).astype(float)
    })
    perf["Excess"] = perf["Ret_gross"] - perf["Bench_ret"]
    perf["NAV_gross"] = (1.0 + perf["Ret_gross"]).cumprod()
    perf["NAV_net"]   = (1.0 + perf["Ret_net"]).cumprod()
    perf["Turnover_money"] = turnover_live.reindex(perf.index).fillna(0.0).astype(float)
    perf["TC_paid"]        = costs_live.reindex(perf.index).fillna(0.0).astype(float)
    if len(regime_at_trade_date):
        reg_ser = pd.Series({pd.Timestamp(k): v for k,v in regime_at_trade_date.items()})
        perf["Regime"] = reg_ser.reindex(perf.index).fillna(method='ffill').fillna("unknown").astype(str)
    else: perf["Regime"] = "unknown"

    try:
        df = perf.copy(); df["Regime"] = df["Regime"].astype(str).str.lower(); _eps = 1e-8
        df["Sign"] = np.where(df["Excess"] > _eps, "pos", np.where(df["Excess"] < -_eps, "neg", "flat"))
        df["Regime_prev"] = df["Regime"].shift(1)
        _p("\n[EXCESS SUMMARY] Win/Loss by regime (live months only)")
        for reg in ["normal", "pre-stress", "stress"]:
            sub = df[df["Regime"] == reg]
            if len(sub) == 0: continue
            n_tot = len(sub); n_pos = int((sub["Sign"] == "pos").sum()); n_neg = int((sub["Sign"] == "neg").sum()); n_flat = int((sub["Sign"] == "flat").sum())
            avg_pos = float(sub.loc[sub["Sign"] == "pos", "Excess"].mean()) if n_pos else np.nan
            avg_neg = float(sub.loc[sub["Sign"] == "neg", "Excess"].mean()) if n_neg else np.nan
            wr = n_pos / n_tot if n_tot else np.nan
            _p(f"  • {reg:10s} | total={n_tot:3d} | win={n_pos:3d} (WR={wr:5.1%}, avg={avg_pos:+.2%}) | loss={n_neg:3d} (avg={avg_neg:+.2%}) | flat={n_flat:3d}")
        trans = df[(df["Regime_prev"].notna()) & (df["Regime_prev"] != df["Regime"])].copy()
        same  = df[(df["Regime_prev"].notna()) & (df["Regime_prev"] == df["Regime"])].copy()
        _p("\n[EXCESS SUMMARY] After regime transitions (current month outcomes)")
        if len(trans):
            tp = int((trans["Sign"] == "pos").sum()); tn = int((trans["Sign"] == "neg").sum()); tf = int((trans["Sign"] == "flat").sum())
            _p(f"  • All transitions | total={len(trans):3d} | pos={tp:3d} | neg={tn:3d} | flat={tf:3d} | avg excess={float(trans['Excess'].mean()):+.2%}")
            for (r0, r1), grp in trans.groupby(["Regime_prev","Regime"]):
                n = len(grp); npos = int((grp["Sign"]=="pos").sum()); nneg = int((grp["Sign"]=="neg").sum()); nflt = n - npos - nneg; avg = float(grp["Excess"].mean())
                _p(f"    - {r0}→{r1:10s} | total={n:3d} | pos={npos:3d} | neg={nneg:3d} | flat={nflt:3d} | avg={avg:+.2%}")
        else: _p("  • No regime transitions observed in live tape.")
        if len(same):
            sp = int((same["Sign"] == "pos").sum()); sn = int((same["Sign"] == "neg").sum()); sf = int((same["Sign"] == "flat").sum())
            _p(f"\n[EXCESS SUMMARY] Same-regime months | total={len(same):3d} | pos={sp:3d} | neg={sn:3d} | flat={sf:3d} | avg excess={float(same['Excess'].mean()):+.2%}\n")
    except Exception as _ex_sum:
        _p(f"[EXCESS SUMMARY] Skipped ({type(_ex_sum).__name__}: {_ex_sum})")

    perf.to_csv(perf_tape_path, index_label="Date")
else:
    pd.DataFrame(columns=["Ret_gross","Ret_net","Bench_ret","Excess","NAV_gross","NAV_net","Turnover_money","TC_paid","Regime"]).to_csv(perf_tape_path, index=False)

summary = {
    "Inception_decision_month": None if 'inception_decision_dt' not in locals() or inception_decision_dt is None else inception_decision_dt.date().isoformat(),
    "Inception_PnL_start_month": None if 'inception_ret_dt' not in locals() or inception_ret_dt is None else inception_ret_dt.date().isoformat(),
    "Months_live_from_inception": int(len(ret_net_live)),
    "CAGR_pct": None if pd.isna(cagr) else round(100*cagr,2),
    "AnnVol_pct": None if pd.isna(vol_ann) else round(100*vol_ann,2),
    "MaxDD_pct": None if pd.isna(mdd) else round(100*mdd,2),
    "Trailing_12M_Return_pct": None if pd.isna(trailing_12m_ret) else round(100*trailing_12m_ret, 2),
    "Turnover_money_ann_pct": None if pd.isna(turnover_ann) else round(100*turnover_ann,2),
    "CostRecovery_x": None if pd.isna(cost_recov_ratio) else round(cost_recov_ratio,2),
    "Artifacts": {
        "last12_positions_csv": last12_path,
        "weights_tape_csv": weights_tape_path,
        "orders_tape_csv": orders_tape_path,
        "performance_tape_csv": perf_tape_path
    },
    "Notes": (
        f"Scoring: normal={CFG3['scoring_map']['normal']} stress={CFG3['scoring_map']['stress']}. "
        "Pre-stress uses direction-aware λ blend. Fallback chain on no-scores. Dynamic vol target on. "
        "Ensemble: IC-EWMA weighted learners (softmax), isotonic on ensemble, vincentized quantiles, conformal on ensemble. "
        "Risk: PCA-conditioned Σ, liquidity diagonal inflation, ADV/AUM vector caps (position & trade), PC1 nudge. "
        "Global per-name cap enforced everywhere. Breadth relaxes (alpha-first) when capacity/vol underutilized."
    )
}
sum_path = os.path.join("./portfolio_outputs", "performance_summary.json")
pd.Series(summary).to_json(sum_path)

_p("\n===== LIVE PORTFOLIO SUMMARY (FROM INCEPTION) =====")
_p(f"Inception decision month: { 'n/a' if 'inception_decision_dt' not in locals() or inception_decision_dt is None else inception_decision_dt.date().isoformat() }")
_p(f"Inception PnL start month: { 'n/a' if 'inception_ret_dt' not in locals() or inception_ret_dt is None else inception_ret_dt.date().isoformat() }")
_p(f"Months (live from inception): {len(ret_net_live)}")
_p(f"CAGR: { 'nan' if pd.isna(cagr) else f'{100*cagr:.2f}%'}")
_p(f"Ann.Vol: { 'nan' if pd.isna(vol_ann) else f'{100*vol_ann:.2f}%'}")
_p(f"Max Drawdown: { 'nan' if pd.isna(mdd) else f'{100*mdd:.2f}%'}")
_p(f"Trailing 12M Return: { 'nan' if pd.isna(trailing_12m_ret) else f'{100*trailing_12m_ret:.2f}%'}")
_p(f"Saved last-12-months positions → {last12_path}")
_p(f"[SAVE] Performance summary: {sum_path}")
_p("==========================================================================\n")

print("First 10 factor names:", list(X_all.columns[:20]))
# -*- coding: utf-8 -*-
# ==========================================================
# Cell 4 — Plot benchmark vs portfolio NAV
# Reads the performance_tape saved by Cell 3 and plots Portfolio NAV vs Benchmark NAV.
# Requirements: matplotlib only (no seaborn), no explicit color settings.
# ==========================================================

import os
import pandas as pd
import matplotlib.pyplot as plt
from typing import Optional, Union

def plot_nav(
    perf_csv_path: str = "./portfolio_outputs/performance_tape.csv",
    use_net: bool = True,
    rebase_to: Optional[float] = 100.0,
    start: Optional[Union[str, pd.Timestamp]] = None,
    end: Optional[Union[str, pd.Timestamp]] = None,
    title: Optional[str] = None,
    save_path: Optional[str] = None,
):
    """
    Plot Portfolio NAV vs Benchmark NAV using the performance tape produced in Cell 3.

    Parameters
    ----------
    perf_csv_path : str
        Path to performance_tape.csv (default: ./portfolio_outputs/performance_tape.csv).
    use_net : bool
        If True, plot NAV_net; else NAV_gross. If the NAV column is missing, it will
        be recomputed from returns (Ret_net/Ret_gross).
    rebase_to : float or None
        Rebase both NAV series to this starting value (first common valid point).
        Set to None to skip rebasing.
    start, end : str or pandas.Timestamp or None
        Optional date filters (inclusive). Example: "2018-01-31" or "2018-01".
    title : str or None
        Optional title. If None, a default title is used.
    save_path : str or None
        If provided, saves the figure to this path (e.g., "./portfolio_outputs/nav_plot.png").

    Returns
    -------
    ax : matplotlib.axes.Axes
        The axes object for further customization.
    """
    if not os.path.exists(perf_csv_path):
        raise FileNotFoundError(f"performance_tape not found at: {perf_csv_path}")

    # Load tape, parse date index
    df = pd.read_csv(perf_csv_path, index_col=0, parse_dates=True)
    if df.empty:
        raise ValueError("performance_tape.csv is empty. Run Cell 3 to generate the tape first.")

    # Optional date filtering
    if start is not None:
        start = pd.to_datetime(start)
        df = df[df.index >= start]
    if end is not None:
        end = pd.to_datetime(end)
        df = df[df.index <= end]

    if df.empty:
        raise ValueError("No data after applying date filters.")

    # Determine which NAV to use (compute if needed)
    nav_col = "NAV_net" if use_net else "NAV_gross"
    if nav_col in df.columns:
        nav = df[nav_col].astype(float)
    else:
        # Recompute from returns if NAV column is missing
        ret_col = "Ret_net" if use_net else "Ret_gross"
        if ret_col not in df.columns:
            raise ValueError(f"Missing both {nav_col} and {ret_col} in performance tape.")
        nav = (1.0 + df[ret_col].astype(float)).cumprod()

    # Benchmark NAV (from benchmark returns)
    if "Bench_ret" not in df.columns:
        raise ValueError("Missing 'Bench_ret' in performance tape.")
    bench_nav = (1.0 + df["Bench_ret"].astype(float)).cumprod()

    # Align and drop any leading NaNs
    bench_nav = bench_nav.reindex(nav.index).astype(float)
    valid = nav.notna() & bench_nav.notna()
    nav = nav[valid]
    bench_nav = bench_nav[valid]

    if len(nav) == 0:
        raise ValueError("No overlapping valid points between portfolio NAV and benchmark NAV.")

    # Rebase if requested
    if rebase_to is not None:
        base_port = float(nav.iloc[0])
        base_bench = float(bench_nav.iloc[0])
        if base_port > 0.0 and base_bench > 0.0:
            nav = nav / base_port * float(rebase_to)
            bench_nav = bench_nav / base_bench * float(rebase_to)

    # Plot (single axes, no subplots; no explicit colors)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(nav.index, nav.values, label=f"Portfolio ({'Net' if use_net else 'Gross'})")
    ax.plot(bench_nav.index, bench_nav.values, label="Benchmark")
    ax.set_xlabel("Date")
    ax.set_ylabel("NAV")
    ax.set_title(title or "Portfolio vs Benchmark — NAV")
    ax.grid(True)
    ax.legend()
    fig.autofmt_xdate()

    if save_path:
        folder = os.path.dirname(save_path)
        if folder:
            os.makedirs(folder, exist_ok=True)
        fig.savefig(save_path, bbox_inches="tight")

    plt.show()
    return ax

# --- Example usage (uncomment to run) ---
ax = plot_nav(
    perf_csv_path="./portfolio_outputs/performance_tape.csv",
    use_net=True,
    rebase_to=100.0,
    start=None,  # e.g., "2018-01"
    end=None,    # e.g., "2024-12"
    title=None,
    save_path="./portfolio_outputs/nav_plot.png",
)
implement the changes here
